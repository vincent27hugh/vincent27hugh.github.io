<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">






  
  
  <link rel="stylesheet" media="all" href="/lib/Han/dist/han.min.css?v=3.3">




<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="machine_learning,natural_language_processing,deep_learning,information_system,markdown_table,image,github,hexo_pdf,finance,economics,mathematics,statistics,artificial_intelligence,open_course,span_id_anchor," />





  <link rel="alternate" href="/atom.xml" title="NEVERLAND" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.jpg?v=5.1.2" />






<meta name="description" content="This is some useful study materials that I collect and I am interested in.">
<meta name="keywords" content="machine_learning,natural_language_processing,deep_learning,information_system,markdown_table,image,github,hexo_pdf,finance,economics,mathematics,statistics,artificial_intelligence,open_course,span_id_anchor">
<meta property="og:type" content="article">
<meta property="og:title" content="2017-18 Open-Course Challenges: Data Science on IS">
<meta property="og:url" content="http://blog.baoduge.com/2017_Study_Plan/index.html">
<meta property="og:site_name" content="NEVERLAND">
<meta property="og:description" content="This is some useful study materials that I collect and I am interested in.">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://img.shields.io/badge/Completed-0-green.svg">
<meta property="og:image" content="https://img.shields.io/badge/OnProcess-13-red.svg">
<meta property="og:image" content="http://progressed.io/bar/95?title=done">
<meta property="og:image" content="http://progressed.io/bar/32?title=done">
<meta property="og:image" content="http://progressed.io/bar/44?title=done">
<meta property="og:image" content="http://progressed.io/bar/37?title=done">
<meta property="og:image" content="http://progressed.io/bar/4?title=done">
<meta property="og:image" content="http://progressed.io/bar/35?title=done">
<meta property="og:image" content="http://progressed.io/bar/11?title=done">
<meta property="og:image" content="http://progressed.io/bar/4?title=done">
<meta property="og:image" content="http://progressed.io/bar/4?title=done">
<meta property="og:image" content="http://progressed.io/bar/6?title=done">
<meta property="og:image" content="http://progressed.io/bar/67?title=done">
<meta property="og:image" content="http://progressed.io/bar/10?title=done">
<meta property="og:image" content="http://progressed.io/bar/54?title=done">
<meta property="og:image" content="http://blog.baoduge.com/images/math_tree.gif">
<meta property="og:image" content="http://blog.baoduge.com/images/Data_Scientist_Track.jpg">
<meta property="og:image" content="http://progressed.io/bar/6?title=done">
<meta property="og:image" content="http://progressed.io/bar/54?title=done">
<meta property="og:image" content="http://progressed.io/bar/67?title=done">
<meta property="og:image" content="http://progressed.io/bar/10?title=done">
<meta property="og:image" content="http://blog.baoduge.com/images/ml_cheatsheet2.png">
<meta property="og:image" content="https://img.shields.io/badge/CheatSheet-AI-blue.svg?logo=github">
<meta property="og:image" content="http://progressed.io/bar/4?title=done">
<meta property="og:image" content="http://progressed.io/bar/4?title=done">
<meta property="og:image" content="http://progressed.io/bar/95?title=done">
<meta property="og:image" content="http://progressed.io/bar/44?title=done">
<meta property="og:image" content="http://progressed.io/bar/32?title=done">
<meta property="og:image" content="http://progressed.io/bar/35?title=done">
<meta property="og:image" content="http://progressed.io/bar/11?title=done">
<meta property="og:image" content="http://progressed.io/bar/37?title=done">
<meta property="og:image" content="http://progressed.io/bar/4?title=done">
<meta property="og:image" content="http://blog.baoduge.com/images/Financial_Theories.jpg">
<meta property="og:image" content="http://blog.baoduge.com/images/Investment_Desicion_Making.jpg">
<meta property="og:updated_time" content="2018-01-19T09:27:04.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="2017-18 Open-Course Challenges: Data Science on IS">
<meta name="twitter:description" content="This is some useful study materials that I collect and I am interested in.">
<meta name="twitter:image" content="https://img.shields.io/badge/Completed-0-green.svg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.2',
    sidebar: {"position":"left","display":"always","offset":12,"offset_float":12,"b2t":true,"scrollpercent":true,"onmobile":true},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://blog.baoduge.com/2017_Study_Plan/"/>





  <title>2017-18 Open-Course Challenges: Data Science on IS | NEVERLAND</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">NEVERLAND</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description">夢幻島</h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://blog.baoduge.com/2017_Study_Plan/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="BDG飽蠹閣">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="http://baoduge.com/My/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="NEVERLAND">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">2017-18 Open-Course Challenges: Data Science on IS</h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-09-12T00:00:00+08:00">
                2017-09-12
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/English/" itemprop="url" rel="index">
                    <span itemprop="name">English</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017_Study_Plan/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2017_Study_Plan/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2017_Study_Plan/" class="leancloud_visitors" data-flag-title="2017-18 Open-Course Challenges: Data Science on IS">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Visitors&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          
              <div class="post-description">
                  This is some useful study materials that I collect and I am interested in.
              </div>
          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        <script src="/assets/js/APlayer.min.js"> </script><p>Inspired by <a href="https://www.scotthyoung.com/blog/myprojects/mit-challenge-2/" target="_blank" rel="external">Scott H.Young - MIT Challenges</a>, I decide to design an Open-course Challenges of 2017-18 for myself. I hope that I could accomplish these goals in time.</p>
<p>Good luck!</p>
<hr>
<h1 id="Statement"><a href="#Statement" class="headerlink" title="Statement"></a>Statement</h1><ol>
<li>I am a year-1 PhD candidate in Information System. Thus the main goal of this challenge is to consolidate the academic foundation and broaden the knowledge. Instead of aim at obtaining a certificate (like on the MOOC), I try to selectively learn the materials which is beneficial for my research and study.</li>
<li>As I&#x2019;m interested in Machine Learning, Financial Engineering and FinTech, these learning materials focus on these areas.</li>
<li>I&#x2019;ve collected all the courses that I think are useful for me and listed them here. However, you don&#x2019;t need to finish all of them (which is almost impossible!). Just select some courses that are beneficial for your research and enjoy it!</li>
</ol>
<hr>
<h1 id="General-Information"><a href="#General-Information" class="headerlink" title="General Information"></a>General Information</h1><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Completed</th>
<th style="text-align:center">On Process</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="https://img.shields.io/badge/Completed-0-green.svg" alt="Completed"></td>
<td style="text-align:center"><img src="https://img.shields.io/badge/OnProcess-13-red.svg" alt="On Process"></td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">NO.</th>
<th style="text-align:center">Course</th>
<th style="text-align:center">Progress</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center"><a href="#OP1">Stanford CS229</a></td>
<td style="text-align:center"><img src="http://progressed.io/bar/95?title=done" alt="19/20"></td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center"><a href="#OP2">UCambridge ITPRNN</a></td>
<td style="text-align:center"><img src="http://progressed.io/bar/32?title=done" alt="5/16"></td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center"><a href="#OP3">Coursera NN4ML</a></td>
<td style="text-align:center"><img src="http://progressed.io/bar/44?title=done" alt="34/78"></td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center"><a href="#OP4">Coursera PGM</a></td>
<td style="text-align:center"><img src="http://progressed.io/bar/37?title=done" alt="35/94"></td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td style="text-align:center"><a href="#OP5">Udacity ML4Trading</a></td>
<td style="text-align:center"><img src="http://progressed.io/bar/4?title=done" alt="1/28"></td>
</tr>
<tr>
<td style="text-align:center">6</td>
<td style="text-align:center"><a href="#OP6">Coursera NLP</a></td>
<td style="text-align:center"><img src="http://progressed.io/bar/35?title=done" alt="37/102"></td>
</tr>
<tr>
<td style="text-align:center">7</td>
<td style="text-align:center"><a href="#OP7">Stanford CS224N DL4NLP</a></td>
<td style="text-align:center"><img src="http://progressed.io/bar/11?title=done" alt="2/18"></td>
</tr>
<tr>
<td style="text-align:center">8</td>
<td style="text-align:center"><a href="#OP8">Berkeley CS188 Intro2AI</a></td>
<td style="text-align:center"><img src="http://progressed.io/bar/4?title=done" alt="1/25"></td>
</tr>
<tr>
<td style="text-align:center">9</td>
<td style="text-align:center"><a href="#OP9">MIT 6.034 AI</a></td>
<td style="text-align:center"><img src="http://progressed.io/bar/4?title=done" alt="1/23"></td>
</tr>
<tr>
<td style="text-align:center">10</td>
<td style="text-align:center"><a href="#OP10">MIT 6.006 Into2Algo</a></td>
<td style="text-align:center"><img src="http://progressed.io/bar/6?title=done" alt="3/47"></td>
</tr>
<tr>
<td style="text-align:center">11</td>
<td style="text-align:center"><a href="#OP11">MIT 18.650 Stat4App</a></td>
<td style="text-align:center"><img src="http://progressed.io/bar/67?title=done" alt="14/22"></td>
</tr>
<tr>
<td style="text-align:center">12</td>
<td style="text-align:center"><a href="#OP12">Stanford StatLearning</a></td>
<td style="text-align:center"><img src="http://progressed.io/bar/10?title=done" alt="1/10"></td>
</tr>
<tr>
<td style="text-align:center">13</td>
<td style="text-align:center"><a href="#OP13">MIT 6.431 PSA&amp;AP</a></td>
<td style="text-align:center"><img src="http://progressed.io/bar/54?title=done" alt="41/76"></td>
</tr>
</tbody>
</table>
</div>
<hr>
<ul>
<li><strong>$\mathbb{UG}$</strong>: Undergraduate Level</li>
<li><strong>$\mathbb{G}$</strong>: Graduate Level</li>
<li><i class="fa fa-play" aria-hidden="true"></i>: You&#x2019;d better watch the videos with normal speed.</li>
<li><i class="fa fa-forward" aria-hidden="true"></i>: You can watch the videos with speed of 1.25X.</li>
<li><i class="fa fa-fast-forward" aria-hidden="true"></i>: You can watch the videos with speed of 1.5X or more.</li>
</ul>
<hr>
<h1 id="Mathematics"><a href="#Mathematics" class="headerlink" title="Mathematics"></a>Mathematics</h1><p><img src="/images/math_tree.gif" alt="Math Tree"></p>
<h2 id="Courses"><a href="#Courses" class="headerlink" title="Courses"></a>Courses</h2><h3 id="Linear-Algebra-amp-Vector-Calculus"><a href="#Linear-Algebra-amp-Vector-Calculus" class="headerlink" title="Linear Algebra &amp; Vector Calculus"></a>Linear Algebra &amp; Vector Calculus</h3><ul>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{UG}$</strong>MIT 18.02 Multivariatble Calculus, Fall 2007, by <strong><a href="https://math.berkeley.edu/~auroux/" target="_blank" rel="external">Dennis Auroux</a></strong></font>: <a href="https://ocw.mit.edu/courses/mathematics/18-02-multivariable-calculus-fall-2007/" target="_blank" rel="external">Course Page</a>, <a href="https://www.youtube.com/playlist?list=PL4C4C8A7D06566F38" target="_blank" rel="external">Videos<i class="fa fa-youtube" aria-hidden="true"></i></a><ul>
<li>Prerequisites: MIT 18.01 - Single Variable Calculus</li>
<li>Course Description: This is a basic subject on matrix theory and linear algebra. Emphasis is given to topics that will be useful in other disciplines, including systems of equations, vector spaces, determinants, eigenvalues, similarity, and positive definite matrices.</li>
</ul>
</li>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{UG}$</strong>MIT 18.06 Linear Algebra by <strong>Gilbert Strang</strong></font>: <a href="https://dspace.mit.edu/handle/1721.1/59010" target="_blank" rel="external">Course Page|Spring2005</a>, <a href="https://www.youtube.com/playlist?list=PLE7DDD91010BC51F8" target="_blank" rel="external">Videos|Spring2005<i class="fa fa-youtube" aria-hidden="true"></i></a>, <a href="https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/" target="_blank" rel="external">Course Page|Fall2011</a>, <a href="https://www.youtube.com/playlist?list=PL221E2BBF13BECF6C" target="_blank" rel="external">Videos-Recitations|Fall2011<i class="fa fa-youtube" aria-hidden="true"></i></a>, <a href="https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/index.htm" target="_blank" rel="external">Course Page|Spring2010</a><ul>
<li>Prerequisites: MIT 18.02 - Multivariable Calculus</li>
<li>Course Description:<ul>
<li>This course covers vector and multi-variable calculus. It is the second semester in the freshman calculus sequence. Topics include vectors and matrices, partial derivatives, double and triple integrals, and vector calculus in 2 and 3-space.</li>
<li>MIT OpenCourseWare offers another version of 18.02, from the Spring 2006 term. Both versions cover the same material, although they are taught by different faculty and rely on different textbooks. Multivariable Calculus (18.02) is taught during the Fall and Spring terms at MIT, and is a required subject for all MIT undergraduates.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Functional-Analysis"><a href="#Functional-Analysis" class="headerlink" title="Functional Analysis"></a>Functional Analysis</h3><ul>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{UG}$</strong>MIT 18.102 - Introduction to Functional Analysis by Richard Melrose, Spring 2009</font>: <a href="https://ocw.mit.edu/courses/mathematics/18-102-introduction-to-functional-analysis-spring-2009/" target="_blank" rel="external">Course Page</a><ul>
<li>Prerequisites:<ul>
<li>Analysis I (18.100)</li>
<li>Linear Algebra (18.06), Linear Algebra (18.700), or Algebra I (18.701)</li>
</ul>
</li>
<li>Course Description: This is a undergraduate course. It will cover normed spaces, completeness, functionals, Hahn-Banach theorem, duality, operators; Lebesgue measure, measurable functions, integrability, completeness of L-p spaces; Hilbert space; compact, Hilbert-Schmidt and trace class operators; as well as spectral theorem.</li>
</ul>
</li>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{UG}$</strong>UCCS Math 535 - Applied Functional Analysis by Greg Morrow</font>: <a href="https://cosmolearning.org/courses/applied-functional-analysis/" target="_blank" rel="external">Course Page</a>, <a href="https://www.youtube.com/playlist?list=PLBC73B96341ECF455" target="_blank" rel="external">Videos<i class="fa fa-youtube" aria-hidden="true"></i></a><ul>
<li>Course Description: This course is an introduction to the basic concepts, methods and applications of functional analysis. Topics covered will include metric spaces, normed spaces, Hilbert spaces, linear operators, spectral theory, fixed point theorems and approximation theorems</li>
</ul>
</li>
</ul>
<h3 id="Numerical-Analysis"><a href="#Numerical-Analysis" class="headerlink" title="Numerical Analysis"></a>Numerical Analysis</h3><ul>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{UG}$</strong>MIT 18.330 Introduction to Numerical Analysis by Laurent Demanet, Spring 2012</font>: <a href="https://ocw.mit.edu/courses/mathematics/18-330-introduction-to-numerical-analysis-spring-2012/" target="_blank" rel="external">Course Page</a><ul>
<li>Prerequisites: Calculus (18.01), Calculus (18.02), and Differential Equations (18.03). Some exposure to linear algebra (matrices) at the level of Linear Algebra (18.06) helps, but is not required. The assignments will involve basic computer programming in the language of your choice (Matlab recommended; this class encourages you to learn Matlab if you don&#x2019;t already know it).</li>
<li>Course Description: This course analyzed the basic techniques for the efficient numerical solution of problems in science and engineering. Topics spanned root finding, interpolation, approximation of functions, integration, differential equations, direct and iterative methods in linear algebra.</li>
</ul>
</li>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{G}$</strong> MIT 18.085 Computational Science and Engineering I by Gilbert Strang, Fall 2008</font>: <a href="https://ocw.mit.edu/courses/mathematics/18-085-computational-science-and-engineering-i-fall-2008/index.htm" target="_blank" rel="external">Course Page</a>, <a href="https://www.youtube.com/playlist?list=PLF706B428FB7BD52C" target="_blank" rel="external">Videos<i class="fa fa-youtube" aria-hidden="true"></i></a><ul>
<li>Prerequisites: Calculus of Several Variables (18.02) and Differential Equations (18.03) or Honors Differential Equations (18.034)</li>
<li>Textbook: Strang, Gilbert. Computational Science and Engineering. Wellesley, MA: Wellesley-Cambridge Press, 2007. ISBN: 9780961408817.</li>
<li>Course Description: This course provides a review of linear algebra, including applications to networks, structures, and estimation, Lagrange multipliers. Also covered are: differential equations of equilibrium; Laplace&#x2019;s equation and potential flow; boundary-value problems; minimum principles and calculus of variations; Fourier series; discrete Fourier transform; convolution; and applications.</li>
<li>Note: This course was previously called &#x201C;Mathematical Methods for Engineers I.&#x201D;</li>
<li>Course Outline: This course has four major topics:<ul>
<li>Applied linear algebra (so important!)</li>
<li>Applied differential equations (for engineering and science)</li>
<li>Fourier methods</li>
<li>Algorithms (lu, qr, eig, svd, finite differences, finite elements, FFT)</li>
</ul>
</li>
</ul>
</li>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{G}$</strong> MIT 18.086 - Mathematical Methods for Engineers II by Gilbert Strang, Spring 2006</font>: <a href="https://ocw.mit.edu/courses/mathematics/18-086-mathematical-methods-for-engineers-ii-spring-2006/index.htm" target="_blank" rel="external">Course Page</a>, <a href="https://www.youtube.com/playlist?list=PL3A13781649466805" target="_blank" rel="external">Videos<i class="fa fa-youtube" aria-hidden="true"></i></a><ul>
<li>Prerequisites: Calculus (18.02), Differential Equations (18.03) or Honors Differential Equations (18.034).</li>
<li>Textbook: Strang, Gilbert. Introduction to Applied Mathematics. Wellesley, MA: Wellesley-Cambridge Press, 1986. ISBN: 9780961408800.</li>
<li>Course Description: This graduate-level course is a continuation of Mathematical Methods for Engineers I (18.085). Topics include numerical methods; initial-value problems; network flows; and optimization.</li>
</ul>
</li>
</ul>
<h3 id="Stochastic-Analysis"><a href="#Stochastic-Analysis" class="headerlink" title="Stochastic Analysis"></a>Stochastic Analysis</h3><ul>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{UG}$</strong>MIT EECS 6.262 - Discrete Stochastic Processes by Robert Gallager, Spring 2011</font>: 1h20min/Lec, 25 Lectures, <a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-262-discrete-stochastic-processes-spring-2011/" target="_blank" rel="external">Course Page</a>, <a href="https://www.youtube.com/playlist?list=PLEEF5322B331C1B98" target="_blank" rel="external">Videos<i class="fa fa-youtube" aria-hidden="true"></i></a><ul>
<li>Prerequisites: MIT 6.041/6.341 Probabilistic Systems Analysis and Applied Probability</li>
<li>Course Description: Discrete stochastic processes are essentially probabilistic systems that evolve in time via random changes occurring at discrete fixed or random intervals. This course aims to help students acquire both the mathematical principles and the intuition necessary to create, analyze, and understand insightful models for a broad range of these processes. The range of areas for which discrete stochastic-process models are useful is constantly expanding, and includes many applications in engineering, physics, biology, operations research and finance.</li>
</ul>
</li>
</ul>
<h3 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h3><ul>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{UG}$</strong>Stanford EE364A: Convex Optimization I by <a href="http://lall.stanford.edu/" target="_blank" rel="external">Sanjay Lall</a></font>: 1h15min/Lec, 19 Lectures, <a href="https://see.stanford.edu/Course/EE364A" target="_blank" rel="external">Course Page 1</a>, <a href="http://ee364a.stanford.edu/" target="_blank" rel="external">Course Page 2</a>, <a href="https://www.youtube.com/playlist?list=PL3940DD956CDF0622" target="_blank" rel="external">Videos<i class="fa fa-youtube" aria-hidden="true"></i></a><ul>
<li>Prerequisites:<ul>
<li>Good knowledge of linear algebra (as in EE263), and exposure to probability. Exposure to numerical computing, optimization, and application fields helpful but not required; the applications will be kept basic and simple.</li>
<li>You will use one of CVX (Matlab), CVXPY (Python), or Convex.jl (Julia), to write simple scripts, so basic familiarity with elementary programming will be required. We refer to CVX, CVXPY, and Convex.jl collectively as CVX.</li>
</ul>
</li>
<li>Course Description: Concentrates on recognizing and solving convex optimization problems that arise in applications. Convex sets, functions, and optimization problems. Basics of convex analysis. Least-squares, linear and quadratic programs, semidefinite programming, minimax, extremal volume, and other problems. Optimality conditions, duality theory, theorems of alternative, and applications. Interior-point methods. Applications to signal processing, statistics and machine learning, control and mechanical engineering, digital and analog circuit design, and finance.</li>
</ul>
</li>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{UG}$</strong>Stanford EE364B: Convex Optimization II by <a href="http://stanford.edu/~boyd/" target="_blank" rel="external">Stephen Boyd</a> and <a href="http://stanford.edu/~jduchi/" target="_blank" rel="external">John Duchi</a></font>: 1h15min/Lec, 18 Lectures, <a href="https://see.stanford.edu/Course/EE364B" target="_blank" rel="external">Course Page 1</a>, <a href="https://stanford.edu/class/ee364b/" target="_blank" rel="external">Course Page 2</a>, <a href="https://www.youtube.com/playlist?list=PL3940DD956CDF0622" target="_blank" rel="external">Videos<i class="fa fa-youtube" aria-hidden="true"></i></a><ul>
<li>Prerequisites: EE364a</li>
<li>Course Description: Continuation of 364a. Subgradient, cutting-plane, and ellipsoid methods. Decentralized convex optimization via primal and dual decomposition. Alternating projections. Exploiting problem structure in implementation. Convex relaxations of hard problems, and global optimization via branch &amp; bound. Robust optimization. Selected applications in areas such as control, circuit design, signal processing, and communications. Course requirements include a project or final exam (chosen individually as desired).</li>
</ul>
</li>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{UG}$</strong>CUHK ENGG 5501 - Foundations of Optimization by <a href="http://www1.se.cuhk.edu.hk/~manchoso/" target="_blank" rel="external">Anthony Man-Cho So</a></font>: <a href="http://www1.se.cuhk.edu.hk/~manchoso/1516/engg5501/" target="_blank" rel="external">Course Page|2015-16</a>, <a href="http://www1.se.cuhk.edu.hk/~manchoso/1718/engg5501/" target="_blank" rel="external">Course Page|2017-18</a><ul>
<li>Course Description: In this course we will develop the basic machinery for formulating and analyzing various optimization problems. Topics include convex analysis, linear and conic linear programming, nonlinear programming, optimality conditions, Lagrangian duality theory, and basics of optimization algorithms. Applications from different fields, such as combinatorial optimization, communications, computational economics and finance, machine learning, and signal and image processing, will be used to complement the theoretical developments. No prior optimization background is required for this class. However, students should have workable knowledge in multivariable calculus, real analysis, linear algebra and matrix theory.</li>
<li>Very detailed lecture notes</li>
</ul>
</li>
</ul>
<h3 id="Financial-Mathematics"><a href="#Financial-Mathematics" class="headerlink" title="Financial Mathematics"></a>Financial Mathematics</h3><ul>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{UG}$</strong>Coursera - Financial Engineering and Risk Management Part I by Martin Haugh and Garud Iyengar</font>: 51 parts, <a href="https://www.coursera.org/learn/financial-engineering-1" target="_blank" rel="external">Course Page</a><ul>
<li>Course Description: Financial Engineering is a multidisciplinary field drawing from finance and economics, mathematics, statistics, engineering and computational methods. The emphasis of FE &amp; RM Part I will be on the use of simple stochastic models to price derivative securities in various asset classes including equities, fixed income, credit and mortgage-backed securities. We will also consider the role that some of these asset classes played during the financial crisis. A notable feature of this course will be an interview module with Emanuel Derman, the renowned ``quant&#x2019;&#x2019; and best-selling author of &#x201C;My Life as a Quant&#x201D;. We hope that students who complete the course will begin to understand the &#x201C;rocket science&#x201D; behind financial engineering but perhaps more importantly, we hope they will also understand the limitations of this theory in practice and why financial models should always be treated with a healthy degree of skepticism. The follow-on course FE &amp; RM Part II will continue to develop derivatives pricing models but it will also focus on asset allocation and portfolio optimization as well as other applications of financial engineering such as real options, commodity and energy derivatives and algorithmic trading.</li>
</ul>
</li>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{UG}$</strong>MIT 18.S096 - Topics in Mathematics with Applications in Finance</font>: 1h20min/Lec, 26 Lectures, <a href="https://ocw.mit.edu/courses/mathematics/18-s096-topics-in-mathematics-with-applications-in-finance-fall-2013/index.htm" target="_blank" rel="external">Course Page</a>, <a href="https://www.youtube.com/playlist?list=PLUl4u3cNGP63ctJIEC1UnZ0btsphnnoHR" target="_blank" rel="external">Videos<i class="fa fa-youtube" aria-hidden="true"></i></a><ul>
<li>Prerequisites:<ul>
<li>MIT 18.01 Single Variable Calculus</li>
<li>MIT 18.02 Multivariable Calculus</li>
<li>MIT 18.03 Differential Equations</li>
<li>MIT 18.05 Introduction to Probability and Statistics or 18.440 Probability and Random Variables</li>
<li>MIT 18.06 Linear Algebra</li>
</ul>
</li>
<li>Course Description: The purpose of the class is to expose undergraduate and graduate students to the mathematical concepts and techniques used in the financial industry. Mathematics lectures are mixed with lectures illustrating the corresponding application in the financial industry. MIT mathematicians teach the mathematics part while industry professionals give the lectures on applications in finance.</li>
</ul>
</li>
</ul>
<hr>
<h1 id="Computer-Science"><a href="#Computer-Science" class="headerlink" title="Computer Science"></a>Computer Science</h1><p><img src="/images/Data_Scientist_Track.jpg" alt="Data Scientist Track"></p>
<h2 id="Courses-1"><a href="#Courses-1" class="headerlink" title="Courses"></a>Courses</h2><h3 id="Data-structure-and-algorithms"><a href="#Data-structure-and-algorithms" class="headerlink" title="Data structure and algorithms"></a>Data structure and algorithms</h3><ul>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{UG}$</strong><span id="OP10"></span><strong>MIT 6.006 - Introduction to Algorithms by <a href="http://erikdemaine.org/" target="_blank" rel="external">Erik Demaine</a> and <a href="https://people.csail.mit.edu/devadas/" target="_blank" rel="external">Srinivas Devadas</a>, Fall 2011</strong></font>: 50min/Lec, 47 Lectures, <a href="https://courses.csail.mit.edu/6.006/fall11/" target="_blank" rel="external">Course Page</a>, <a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-006-introduction-to-algorithms-fall-2011/" target="_blank" rel="external">Course Page|MIT OpenCourseWare</a>, <a href="https://www.youtube.com/playlist?list=PLUl4u3cNGP61Oq3tWYp6V_F-5jb5L2iHb" target="_blank" rel="external">Videos<i class="fa fa-youtube" aria-hidden="true"></i></a> <i class="fa fa-forward" aria-hidden="true"></i> <img src="http://progressed.io/bar/6?title=done" alt="3/47"><ul>
<li>Prerequisite: A firm grasp of Python and a solid background in discrete mathematics are necessary prerequisites to this course. You are expected to have mastered the material presented in 6.01 Introduction to EECS I and 6.042J Mathematics for Computer Science.</li>
<li>Course Description: This course provides an introduction to mathematical modeling of computational problems. It covers the common algorithms, algorithmic paradigms, and data structures used to solve these problems. The course emphasizes the relationship between algorithms and programming, and introduces basic performance measures and analysis techniques for these problems.</li>
</ul>
</li>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{UG}$</strong>MIT 6.046J - Design and Analysis of Algorithms by <a href="http://erikdemaine.org/" target="_blank" rel="external">Erik Demaine</a>, <a href="https://people.csail.mit.edu/devadas/" target="_blank" rel="external">Srinivas Devadas</a> and Nancy Lynch, Spring 2015</font>: <a href="http://stellar.mit.edu/S/course/6/sp15/6.046J/index.html" target="_blank" rel="external">Course Page</a>, <a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-design-and-analysis-of-algorithms-spring-2015/" target="_blank" rel="external">Course Page|MIT OpenCourseWare</a>, <a href="https://www.youtube.com/playlist?list=PLUl4u3cNGP6317WaSNfmCvGym2ucw3oGp" target="_blank" rel="external">Videos<i class="fa fa-youtube" aria-hidden="true"></i></a><ul>
<li>Prerequisites: This course is the header course for the Theory of Computation concentration. You are expected, and strongly encouraged, to have taken:<ul>
<li>6.006 Introduction to Algorithms</li>
<li>6.042J / 18.062J Mathematics for Computer Science</li>
</ul>
</li>
<li>Course Description: This is an intermediate algorithms course with an emphasis on teaching techniques for the design and analysis of efficient algorithms, emphasizing methods of application. Topics include divide-and-conquer, randomization, dynamic programming, greedy algorithms, incremental improvement, complexity, and cryptography.</li>
</ul>
</li>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{G}$</strong>MIT 18.409 - Algorithmic Aspects of Machine Learning by Ankur Moitra, Spring 2015</font>: <a href="http://people.csail.mit.edu/moitra/409.html" target="_blank" rel="external">Course Page</a>,  <a href="https://ocw.mit.edu/courses/mathematics/18-409-algorithmic-aspects-of-machine-learning-spring-2015/" target="_blank" rel="external">Course Page|MIT OpenCourseWare</a>, <a href="https://www.youtube.com/playlist?list=PLB3sDpSRdrOvI1hYXNsa6Lety7K8FhPpx" target="_blank" rel="external">Videos<i class="fa fa-youtube" aria-hidden="true"></i></a><ul>
<li>Prerequisites: You will need a strong background in algorithms, probability and linear algebra.<ul>
<li>6.046J / 18.410J Design and Analysis of Algorithms or equivalent</li>
<li>6.041SC Probabilistic Systems Analysis and Applied Probability or 18.440 Probability and Random Variables or equivalent.</li>
</ul>
</li>
<li>Course Description: This course is organized around algorithmic issues that arise in machine learning. Modern machine learning systems are often built on top of algorithms that do not have provable guarantees, and it is the subject of debate when and why they work. In this class, we focus on designing algorithms whose performance we can rigorously analyze for fundamental machine learning problems.</li>
</ul>
</li>
</ul>
<hr>
<h1 id="Statistics-amp-Probability"><a href="#Statistics-amp-Probability" class="headerlink" title="Statistics &amp; Probability"></a>Statistics &amp; Probability</h1><p>Probability Cheatsheet:</p>


	<div class="row">
		<iframe src="https://drive.google.com/file/d/0ByuqsS9ThqpcbHh4TV9jeTZnZ3c/preview" style="width:100%; height:550px"></iframe>
	</div>



<p>Statistics Cheatsheet:</p>


	<div class="row">
		<iframe src="https://drive.google.com/file/d/0ByuqsS9ThqpcTUZqRk9DbmoydDA/preview" style="width:100%; height:550px"></iframe>
	</div>



<h2 id="Courses-2"><a href="#Courses-2" class="headerlink" title="Courses"></a>Courses</h2><h3 id="Basics"><a href="#Basics" class="headerlink" title="Basics"></a>Basics</h3><ul>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{UG}$</strong><span id="OP13"></span>MIT 6.041(UG)/6.431(G) - Probabilistic Systems Analysis and Applied Probability by <a href="http://www.mit.edu/~jnt/home.html" target="_blank" rel="external"><strong>John Tsitsiklis</strong></a>, Fall 2010</font>: 25 Lectures, <a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/index.htm" target="_blank" rel="external">Course Page</a>, <a href="https://www.youtube.com/playlist?list=PLUl4u3cNGP60A3XMwZ5sep719_nh95qOe" target="_blank" rel="external">Videos<i class="fa fa-youtube" aria-hidden="true"></i></a> <i class="fa fa-fast-forward" aria-hidden="true"></i> <img src="http://progressed.io/bar/54?title=done" alt="41/76"><ul>
<li>Prerequisites: MIT 18.02 - Multivariable Calculus</li>
<li>Textbook: Bertsekas, Dimitri, and John Tsitsiklis. Introduction to Probability. 2nd ed. Athena Scientific, 2008. ISBN: 9781886529236.</li>
<li>Course Description: Welcome to 6.041/6.431, a subject on the modeling and analysis of random phenomena and processes, including the basics of statistical inference. Nowadays, there is broad consensus that the ability to think probabilistically is a fundamental component of scientific literacy. The aim of this class is to introduce the relevant models, skills, and tools, by combining mathematics with conceptual understanding and intuition.</li>
</ul>
</li>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{UG}$</strong> MIT 18.440 - Probability and Random Variables by <strong><a href="http://math.mit.edu/~sheffield/" target="_blank" rel="external">Scott Sheffield</a></strong>, Spring 2014</font>: <a href="https://ocw.mit.edu/courses/mathematics/18-440-probability-and-random-variables-spring-2014/index.htm" target="_blank" rel="external">Course Page</a><ul>
<li>Prerequisites: 18.02SC Multivariable Calculus</li>
<li>Textbook: Ross, Sheldon. A First Course in Probability. 8th ed. Pearson Prentice Hall, 2009. ISBN: 9780136033134.</li>
<li>Course Description: This course introduces students to probability and random variables. Topics include distribution functions, binomial, geometric, hypergeometric, and Poisson distributions. The other topics covered are uniform, exponential, normal, gamma and beta distributions; conditional probability; Bayes theorem; joint distributions; Chebyshev inequality; law of large numbers; and central limit theorem.</li>
</ul>
</li>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{UG}$</strong>/<strong>$\mathbb{G}$</strong><span id="OP11"></span><strong>MIT 18.650 - Statistics for Applications by Prof. Philippe Rigollet, Fall 2016</strong></font>: 1h15min/Lec, 21 Lectures, <a href="https://ocw.mit.edu/courses/mathematics/18-650-statistics-for-applications-fall-2016/" target="_blank" rel="external">Course Page</a>, <a href="https://www.youtube.com/playlist?list=PLUl4u3cNGP60uVBMaoNERc6knT_MgPKS0" target="_blank" rel="external">Videos<i class="fa fa-youtube" aria-hidden="true"></i></a> <i class="fa fa-play" aria-hidden="true"></i> <img src="http://progressed.io/bar/67?title=done" alt="14/22"><ul>
<li>Prerequisites: Probability theory at the level of <a href="https://ocw.mit.edu/courses/mathematics/18-440-probability-and-random-variables-spring-2014/" target="_blank" rel="external"><strong>18.440 Probability and Random Variables</strong></a>. Some linear algebra (matrices, vectors, eigenvalues).</li>
<li>Course Description: This course offers an in-depth the theoretical foundations for statistical methods that are useful in many applications. The goal is to understand the role of mathematics in the research and development of efficient statistical methods.</li>
<li>Rigollet&#x2019;s style is very competent, so you are suggested to watch the video in normal speed.</li>
</ul>
</li>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{G}$</strong> CMU 36-401 Modern Regression by <strong><a href="http://www.stat.cmu.edu/~larry/" target="_blank" rel="external">Larry Wasserman</a></strong></font>: <a href="http://www.stat.cmu.edu/~larry/=stat401/" target="_blank" rel="external">Course Page</a><ul>
<li>Prerequisites: A minimum grade of C in any one of the pre-requisites is required. A grade of C is required to move on to 36-402 or any 36-46x course.<ul>
<li>At least a C grade in (36-226 or 36-625 or 73-407 or 36-310) and (21-240 or 21-241).</li>
</ul>
</li>
<li>Textbook: Applied Linear Regression Models, Fourth Edition by Kutner, Nachtsheim and Neter.</li>
<li>Course Description: This course is an introduction to applied data analysis. We will explore data sets, examine various models for the data, assess the validity of their assumptions, and determine which conclusions we can make (if any). Data analysis is a bit of an art; there may be several valid approaches. We will strongly emphasize the importance of critical thinking about the data and the question of interest. Our overall goal is to use a basic set of modeling tools to explore and analyze data and to present the results in a scientific report. The course includes a review and discussion of exploratory methods, informal techniques for summarizing and viewing data. We then consider simple linear regression, a model that uses only one predictor. After briefly reviewing some linear algebra, we turn to multiple linear regression, a model that uses multiple variables to predict the response of interest. For all models, we will examine the underlying assumptions. More specifically, do the data support the assumptions? Do they contradict them? What are the consequences for inference? Finally, we will explore extra topics such as nonlinear regression or regression with time-dependent data.</li>
</ul>
</li>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{G}$</strong> CMU 36-705 Intermediate Statistics by <strong><a href="http://www.stat.cmu.edu/~larry/" target="_blank" rel="external">Larry Wasserman</a></strong></font>: <a href="http://www.stat.cmu.edu/~larry/=stat705/" target="_blank" rel="external">Course Page</a>, <a href="https://www.youtube.com/playlist?list=PLNhdfSlxbk-ZC7ipZ_EzhyX7TKrVunWE6" target="_blank" rel="external">Videos<i class="fa fa-youtube" aria-hidden="true"></i></a><ul>
<li>Textbook: Wasserman, L. (2004). All of Statistics: A concise course in statistical inference.</li>
<li>Prerequisites: I assume that you know the material in Chapters 1-3 of of the book (basic probability) are familiar to you. If not, then you should take 36-700 Probability and Mathematical Statistics I.</li>
<li>Course Description: This course will cover the fundamentals of theoretical statistics. We will cover Chapters 1 &#x2014; 12 from the text plus some supplementary material. This course is excellent preparation for advanced work in statistics and machine learning.</li>
</ul>
</li>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{G}$</strong>MIT 18.175 - Theory of Probability by <strong><a href="http://math.mit.edu/~sheffield/" target="_blank" rel="external">Scott Sheffield</a></strong>, Spring 2014</font>: <a href="https://ocw.mit.edu/courses/mathematics/18-175-theory-of-probability-spring-2014/" target="_blank" rel="external">Course Page</a><ul>
<li>Prerequisites: <a href="https://ocw.mit.edu/courses/mathematics/18-100c-real-analysis-fall-2012/" target="_blank" rel="external">MIT 18.100C Real Analysis</a></li>
<li>Course Description: This course covers topics such as sums of independent random variables, central limit phenomena, infinitely divisible laws, Levy processes, Brownian motion, conditioning, and martingales.</li>
</ul>
</li>
</ul>
<h3 id="Statistical-Learning"><a href="#Statistical-Learning" class="headerlink" title="Statistical Learning"></a>Statistical Learning</h3><ul>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{G}$</strong><span id="OP12"></span>Stanford Online-StatLearning Statistical Learning by <a href="https://web.stanford.edu/~hastie/" target="_blank" rel="external"><strong>Trevor Hastie</strong></a> and <a href="https://statweb.stanford.edu/~tibs/" target="_blank" rel="external"><strong>Rob Tibshirani</strong></a></font>: 10 Lectures, <a href="https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/Winter2016/about" target="_blank" rel="external">Course Page</a>, <a href="https://github.com/asadoughi/stat-learning" target="_blank" rel="external">Notes&amp;Excercise<i class="fa fa-github" aria-hidden="true"></i></a> <i class="fa fa-play" aria-hidden="true"></i> <img src="http://progressed.io/bar/10?title=done" alt="1/10"><ul>
<li>Prerequisites: First courses in statistics, linear algebra, and computing.</li>
<li>Textbook: James, Gareth, et al. An introduction to statistical learning. Vol. 112. New York: springer, 2013.</li>
<li>Course Description:<ul>
<li>This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).</li>
<li>This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data analysis. Computing is done in R. There are lectures devoted to R, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.</li>
</ul>
</li>
</ul>
</li>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{G}$</strong>CMU 10-702/36-702 - Statistical Machine Learning by <a href="http://www.stat.cmu.edu/~larry/" target="_blank" rel="external"><strong>Larry Wasserman</strong></a>, Spring 2016</font>: 1h15min, 24 Lectures, <a href="http://www.stat.cmu.edu/~larry/=sml/" target="_blank" rel="external">Course Page</a>, <a href="https://www.youtube.com/playlist?list=PLTB9VQq8WiaCBK2XrtYn5t9uuPdsNm7YE" target="_blank" rel="external">Videos<i class="fa fa-youtube" aria-hidden="true"></i></a><ul>
<li>Prerequisites: You should have taken <strong><a href="http://www.cs.cmu.edu/~epxing/Class/10701/" target="_blank" rel="external">10-701 Introduction to Machine Learning</a></strong> and <strong>36-715 Discrete Time Stochastic Processes</strong>. If you did not take these courses, it is your responsibility to do background reading to make sure you understand the concepts in those courses. We will assume that you are familiar with the following concepts:<ol>
<li>Convergence in probability and convergence in distribution.</li>
<li>The central limit theorem and the law of large numbers.</li>
<li>Maximum likelihood, Fisher information.</li>
<li>Bayesian inference.</li>
<li>Regression.</li>
<li>The Bias-Variance tradeoff.</li>
<li>Bayes classifiers; linear classifiers; support vector machines.</li>
<li>Determinants, eigenvalues and eigenvectors.</li>
</ol>
</li>
<li>Course Description: Statistical Machine Learning is a second graduate level course in advanced machine learning, assuming students have taken <strong>Machine Learning (10-715)</strong> - <a href="https://www.cs.cmu.edu/~epxing/Class/10715/" target="_blank" rel="external">2014Fall</a>, <a href="http://www.cs.cmu.edu/~bapoczos/Classes/ML10715_2015Fall/" target="_blank" rel="external">2015Fall</a> and <strong><a href="http://www.stat.cmu.edu/~larry/=stat705/" target="_blank" rel="external">Intermediate Statistics (36-705)</a></strong>. The term &#x201C;statistical&#x201D; in the title reflects the emphasis on statistical theory and methodology. The course combines methodology with theoretical foundations. Theorems are presented together with practical aspects of methodology and intuition to help students develop tools for selecting appropriate methods and approaches to problems in their own research. The course includes topics in statistical theory that are important for researchers in machine learning, including nonparametric theory, consistency, minimax estimation, and concentration of measure.</li>
</ul>
</li>
</ul>
<h3 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h3><ul>
<li><strong>$\mathbb{UG}$</strong>Charles University in Prague NMFM404 Selected Software Tools for Finance and Insurance by Michal Pe&#x161;ta: <a href="http://www.karlin.mff.cuni.cz/~pesta/NMFM404-1617.html" target="_blank" rel="external">Course Page</a><ul>
<li>Course Structure:<ul>
<li>Robust regression</li>
<li>Logistic regression and Exact logistic regression</li>
<li>Multinomial logistic regression and Ordinal logistic regression</li>
<li>Probit regression</li>
<li>Poisson regression and Negative binomial regression</li>
<li>Zero-inflated Poisson regression and Zero-inflated NB regression</li>
<li>Zero-truncated Poisson regression and Zero-truncated NB regression</li>
<li>Survival data analysis</li>
<li>Proportional hazards</li>
<li>Tobit regression</li>
<li>Truncated regression and Interval regression</li>
<li>Generalized linear mixed models</li>
<li>Rector&#x2019;s day</li>
<li>Mixed effects logistic regression</li>
</ul>
</li>
</ul>
</li>
<li><strong>$\mathbb{UG}$</strong>ISU - STAT 544 Introduction to Bayesian statistics by <a href="http://www.jarad.me/" target="_blank" rel="external">Jarad Niemi</a>: UG, 28 Lectures, <a href="http://www.jarad.me/courses/stat544/" target="_blank" rel="external">Course Page</a>, <a href="https://www.youtube.com/playlist?list=PLUAHeOPjkJscoDA4FZXN_h6Vj_1V5AdUi" target="_blank" rel="external">Videos<i class="fa fa-github" aria-hidden="true"></i></a><ul>
<li>Prerequisite: STAT 543 or equivalent, e.g. Econ 672.</li>
<li>Textbook: Gelman, Andrew, et al. Bayesian data analysis. Vol. 2. Boca Raton, FL: CRC press, 2014.</li>
<li>Course Description: Specification of probability models; subjective, conjugate, and noninformative prior distributions; hierarchical models; analytical and computational techniques for obtaining posterior distributions; model checking, model selection, diagnostics; comparison of Bayesian and traditional methods.</li>
</ul>
</li>
<li><strong>$\mathbb{UG}$</strong>Statistics by <a href="https://www.youtube.com/channel/UCoHhuummRZaIVX7bD4t2czg" target="_blank" rel="external">@Professor Lenard on YouTube</a>: 28 videos 923K+ views, <a href="https://www.youtube.com/playlist?list=PL5102DFDC6790F3D0" target="_blank" rel="external">Videos<i class="fa fa-youtube" aria-hidden="true"></i></a></li>
<li><strong>$\mathbb{UG}$</strong>MIT 18.05 Introduction to Probability and Statistics by Jeremy Orloff and Jonathan Bloom, Spring 2014: <a href="https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/" target="_blank" rel="external">Course Page</a><ul>
<li>Course Description: This course provides an elementary introduction to probability and statistics with applications. Topics include: basic combinatorics, random variables, probability distributions, Bayesian inference, hypothesis testing, confidence intervals, and linear regression.</li>
</ul>
</li>
<li><strong>$\mathbb{UG}$</strong>Harvard Stat 110 Probability by Joe Blitzstein, : <a href="https://projects.iq.harvard.edu/stat110" target="_blank" rel="external">Course Page</a>, <a href="https://www.youtube.com/playlist?list=PL2SOU6wwxB0uwwH80KTQ6ht66KWxbzTIo" target="_blank" rel="external">Videos<i class="fa fa-youtube" aria-hidden="true"></i></a>, <a href="https://github.com/buruzaemon/stats-110" target="_blank" rel="external">GitHub<i class="fa fa-github" aria-hidden="true"></i></a><ul>
<li>Joe Blitzstein (Professor of the Practice in Statistics and Co-Director of Undergraduate Studies in Statistics, Harvard University) has taught Statistics 110: Probability at Harvard each year since 2006. The on-campus Stat 110 course has grown from 80 students to over 300 students per year in that time. The lecture videos are available on iTunes U and YouTube.</li>
<li>Course Description: Stat 110 is an introduction to probability as a language and set of tools for understanding statistics, science, risk, and randomness. The ideas and methods are useful in statistics, science, engineering, economics, finance, and everyday life. Topics include the following. Basics: sample spaces and events, conditioning, Bayes&#x2019; Theorem. Random variables and their distributions: distributions, moment generating functions, expectation, variance, covariance, correlation, conditional expectation. Univariate distributions: Normal, t, Binomial, Negative Binomial, Poisson, Beta, Gamma. Multivariate distributions: joint, conditional, and marginal distributions, independence, transformations, Multinomial, Multivariate Normal. Limit theorems: law of large numbers, central limit theorem. Markov chains: transition probabilities, stationary distributions, reversibility, convergence. The prerequisites are calculus (mainly single variable) and familiarity with matrices.</li>
</ul>
</li>
<li><strong>$\mathbb{G}$</strong> MIT 15.075J - Statistical Thinking and Data Analysis by Cynthia Rudin, Fall 2011: <a href="https://ocw.mit.edu/courses/sloan-school-of-management/15-075j-statistical-thinking-and-data-analysis-fall-2011/" target="_blank" rel="external">Course Page</a><ul>
<li>Prerequisites: An understanding of Calculus and 6.041 Probabilistic Systems Analysis and Applied Probability or 18.440 Probability and Random Variables.</li>
<li>Textbook: Tamhane, Ajit C., and Dorothy D. Dunlop. Statistics and Data Analysis: From Elementary to Intermediate. Prentice Hall, 1999. ISBN: 9780137444267.</li>
<li>Course Description: This course is an introduction to statistical data analysis. Topics are chosen from applied probability, sampling, estimation, hypothesis testing, linear regression, analysis of variance, categorical data analysis, and nonparametric statistics.</li>
</ul>
</li>
<li><strong>$\mathbb{G}$</strong>MIT 18.465 - Topics in Statistics: Nonparametrics and Robustness by Prof. Richard Dudley, Spring 2005: <a href="https://ocw.mit.edu/courses/mathematics/18-465-topics-in-statistics-nonparametrics-and-robustness-spring-2005/" target="_blank" rel="external">Course Page</a><ul>
<li>Prerequisites: Statistics for Applications (18.443) or Statistical Inference (the former 18.441)</li>
<li>Course Description:<ul>
<li>This graduate-level course focuses on one-dimensional nonparametric statistics developed mainly from around 1945 and deals with order statistics and ranks, allowing very general distributions.</li>
<li>For multidimensional nonparametric statistics, an early approach was to choose a fixed coordinate system and work with order statistics and ranks in each coordinate. A more modern method, to be followed in this course, is to look for rotationally or affine invariant procedures. These can be based on empirical processes as in computer learning theory.</li>
<li>Robustness, which developed mainly from around 1964, provides methods that are resistant to errors or outliers in the data, which can be arbitrarily large. Nonparametric methods tend to be robust.</li>
</ul>
</li>
</ul>
</li>
<li><strong>$\mathbb{G}$</strong>MIT 18.465 - Topics in Statistics: Statistical Learning Theory by Prof. Dmitry Panchenko, Spring 2007: <a href="https://ocw.mit.edu/courses/mathematics/18-465-topics-in-statistics-statistical-learning-theory-spring-2007/" target="_blank" rel="external">Course Page</a><ul>
<li>Prerequisites: Theory of Probability (18.175) and either Statistical Learning Theory and Applications (9.520) or Machine Learning (6.867)</li>
<li>Course Description: The main goal of this course is to study the generalization ability of a number of popular machine learning algorithms such as boosting, support vector machines and neural networks. Topics include Vapnik-Chervonenkis theory, concentration inequalities in product spaces, and other elements of empirical process theory.</li>
</ul>
</li>
<li><strong>$\mathbb{G}$</strong>MIT 18.655 - Mathematical Statistics by Dr. Peter Kempthorne, Spring 2016: <a href="https://ocw.mit.edu/courses/mathematics/18-655-mathematical-statistics-spring-2016/" target="_blank" rel="external">Course Page</a><ul>
<li>Textbook: Bickel, Peter J., and Kjell A. Doksum. Mathematical Statistics: Basic Ideas and Selected Topics, Volume 1. 2nd edition. Chapman and Hall / CRC, 2015. ISBN: 9781498723800.</li>
<li>Course Description: This course provides students with decision theory, estimation, confidence intervals, and hypothesis testing. It introduces large sample theory, asymptotic efficiency of estimates, exponential families, and sequential analysis.</li>
</ul>
</li>
</ul>
<h2 id="Books"><a href="#Books" class="headerlink" title="Books"></a>Books</h2><ul>
<li><strong>An Introduction to Statistical Learning: with Applications in R by G. James, D. Witten, T. Hastie, R. Tibshirani</strong>: <a href="https://www.amazon.com/Introduction-Statistical-Learning-Applications-Statistics/dp/1461471370/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1509885827&amp;sr=1-1&amp;keywords=ISLR" target="_blank" rel="external">Amazon<i class="fa fa-amazon" aria-hidden="true"></i></a>, <a href="http://www-bcf.usc.edu/~gareth/ISL/index.html" target="_blank" rel="external">Home Page<i class="fa fa-home" aria-hidden="true"></i></a></li>
<li><strong>The Elements of Statistical Learning by T. Hastie, R. Tibshirani, J. Friedman</strong>: <a href="https://www.amazon.com/Elements-Statistical-Learning-Prediction-Statistics/dp/0387848576/ref=pd_sim_14_1?_encoding=UTF8&amp;pd_rd_i=0387848576&amp;pd_rd_r=GK2X9NEZ98X3C69X24BF&amp;pd_rd_w=UwxYI&amp;pd_rd_wg=PRX5u&amp;psc=1&amp;refRID=GK2X9NEZ98X3C69X24BF" target="_blank" rel="external">Amazon<i class="fa fa-amazon" aria-hidden="true"></i></a>, <a href="1.Machine Learning/ESL.pdf">E-Book</a></li>
<li>&#x7EDF;&#x8BA1;&#x5B66;&#x4E60;&#x65B9;&#x6CD5; by <a href="https://baike.baidu.com/item/%E6%9D%8E%E8%88%AA/266587" target="_blank" rel="external">&#x674E;&#x822A;</a>: <a href="https://www.amazon.cn/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%9D%8E%E8%88%AA/dp/B007TSFMTA" target="_blank" rel="external">Amazon<i class="fa fa-amazon" aria-hidden="true"></i></a>, <a href="https://book.douban.com/subject/10590856/" target="_blank" rel="external">&#x8C46;&#x74E3;&#x8BFB;&#x4E66;</a>, <a href="https://github.com/peimin/li_hang" target="_blank" rel="external">Slide<i class="fa fa-github" aria-hidden="true"></i></a>, <a href="https://github.com/WenDesi/lihang_book_algorithm" target="_blank" rel="external">Codes of Python<i class="fa fa-github" aria-hidden="true"></i></a></li>
<li>Introduction to Probability and Statistics Using R (IPSUR) by G. Jay Kerns: <a href="https://www.amazon.com/Introduction-Probability-Statistics-Using-R/dp/0557249791/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1509886058&amp;sr=1-1&amp;keywords=Introduction+to+Probability+and+Statistics+Using+R" target="_blank" rel="external">Amazon<i class="fa fa-amazon" aria-hidden="true"></i></a>, <a href="http://ipsur.org/" target="_blank" rel="external">Book Page<i class="fa fa-home" aria-hidden="true"></i></a>, <a href="https://github.com/gjkerns/IPSUR" target="_blank" rel="external">GitHub<i class="fa fa-github" aria-hidden="true"></i></a></li>
</ul>
<hr>
<h1 id="Machine-Learning"><a href="#Machine-Learning" class="headerlink" title="Machine Learning"></a>Machine Learning</h1><p><img src="/images/ml_cheatsheet2.png" alt="Machine Learning Cheat Sheet"></p>
<p><img src="https://img.shields.io/badge/CheatSheet-AI-blue.svg?logo=github" alt="">Essential Cheat Sheets for deep learning and machine learning researchers: <a href="https://github.com/kailashahirwar/cheatsheets-ai" target="_blank" rel="external">GitHub<i class="fa fa-github" aria-hidden="true"></i></a></p>
<h2 id="Courses-3"><a href="#Courses-3" class="headerlink" title="Courses"></a>Courses</h2><h3 id="Artificial-Intelligence"><a href="#Artificial-Intelligence" class="headerlink" title="Artificial Intelligence"></a>Artificial Intelligence</h3><ol>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{UG}$</strong><span id="OP8"></span>Berkeley CS188 Introduction to Artificial Intelligence by <a href="https://people.eecs.berkeley.edu/~pabbeel/" target="_blank" rel="external"><strong>Pieter Abbeel</strong></a>,Spring 2014</font>: 25 Lectures, <a href="https://people.eecs.berkeley.edu/~russell/classes/cs188/f14/" target="_blank" rel="external">Course Page</a>, <a href="https://www.youtube.com/playlist?list=PL6MuV0DF6AuoviA41dtji6q-PM4hvAcNk" target="_blank" rel="external">Videos<i class="fa fa-youtube" aria-hidden="true"></i></a> <i class="fa fa-forward" aria-hidden="true"></i> <img src="http://progressed.io/bar/4?title=done" alt="1/25"><ul>
<li>Prerequisites<ul>
<li>CS 61A Structure and Interpretation of Computer Programs and 61B Data Structure: Prior computer programming experience and a good understanding of clean, elegant implementations of nontrivial algorithms and data structures.</li>
<li>CS 70 Discrete Mathematics and Probability Theory: Facility with basic concepts of propositional logic and probability are expected, as well as the ability to understand and construct proofs. CS 70 is the better choice for this course, but Math 55 could be substituted. Some linear algebra and calculus will be used, but the necessary content will be covered in the course.</li>
</ul>
</li>
<li>Course Description: This course will cover Ideas and techniques underlying the design of intelligent computer systems. Topics include search, game playing, knowledge representation, inference, planning, reasoning under uncertainty, machine learning, robotics, perception, and language understanding. You will learn to build intelligent agents - systems that perceive and act - for fully observable, partially observable and adversarial settings. Your agents will generate provably successful plans in deterministic environments, draw inferences in uncertain environments, and optimize actions for arbitrary reward structures. They will learn from observation and from rewards. The techniques you learn in this course apply to a wide variety of artificial intelligence problems and will serve as the foundation for further study in any application area you choose to pursue.</li>
</ul>
</li>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{UG}$</strong><span id="OP9"></span><strong>MIT 6.034 Artificial Intelligence</strong> by <strong>Patrick H. Winston</strong>, Fall 2010</font>: 23 Lectures, <a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/" target="_blank" rel="external">Course Page</a>, <a href="https://www.youtube.com/playlist?list=PLUl4u3cNGP63gFHB6xb-kVBiQHYe_4hSi" target="_blank" rel="external">Videos<i class="fa fa-youtube" aria-hidden="true"></i></a> <i class="fa fa-forward" aria-hidden="forward"></i> <img src="http://progressed.io/bar/4?title=done" alt="1/23"><ul>
<li>Course Description: This course introduces students to the basic knowledge representation, problem solving, and learning methods of artificial intelligence. Upon completion of 6.034, students should be able to develop intelligent systems by assembling solutions to concrete computational problems; understand the role of knowledge representation, problem solving, and learning in intelligent-system engineering; and appreciate the role of problem solving, vision, and language in understanding human intelligence from a computational perspective.</li>
</ul>
</li>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{UG}$</strong>Udacity|Intro to Artificial Intelligence by <strong>Sebastian Thrun</strong></font>: <a href="https://www.udacity.com/course/intro-to-artificial-intelligence--cs271" target="_blank" rel="external">Course Page</a>, <a href="https://www.youtube.com/playlist?list=PLAwxTw4SYaPlqMkzr4xyuD6cXTIgPuzgn" target="_blank" rel="external">Videos<i class="fa fa-youtube" aria-hidden="true"></i></a><ul>
<li>Course Description: Artificial Intelligence (AI) is a field that has a long history but is still constantly and actively growing and changing. In this course, you&#x2019;ll learn the basics of modern AI as well as some of the representative applications of AI. Along the way, we also hope to excite you about the numerous applications and huge possibilities in the field of AI, which continues to expand human capability beyond our imagination.</li>
</ul>
</li>
</ol>
<h3 id="Machine-Learning-1"><a href="#Machine-Learning-1" class="headerlink" title="Machine Learning"></a>Machine Learning</h3><ol>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{UG}$</strong><span id="OP1"></span>Stanford CS229 - Machine Learning by <a href="http://www.andrewng.org/" target="_blank" rel="external"><strong>Andrew Ng</strong></a></font>: 1h15min/Lec, 20 Lectures, <a href="http://cs229.stanford.edu/" target="_blank" rel="external">Course Page</a>, <a href="https://see.stanford.edu/Course/CS229/47" target="_blank" rel="external">Videos-1<i class="fa fa-external-link" aria-hidden="true"></i></a>, <a href="https://www.youtube.com/playlist?list=PLA89DCFA6ADACE599" target="_blank" rel="external">Videos-2<i class="fa fa-youtube" aria-hidden="true"></i></a>, <a href="https://github.com/CourseAce/Stanford-MachineLearning" target="_blank" rel="external">Code-Matlab|CourseAce<i class="fa fa-github" aria-hidden="true"></i></a>, <a href="icrtiou/Coursera-ML-AndrewNg">Code-Python|icrtiou<i class="fa fa-github" aria-hidden="true"></i></a> <i class="fa fa-play" aria-hidden="true"></i> <img src="http://progressed.io/bar/95?title=done" alt="19/20"><ul>
<li>One of the most classic open course for Machine Learning</li>
<li>Prerequisites: Students are expected to have the following background:<ul>
<li>Knowledge of basic computer science principles and skills, at a level sufficient to write a reasonably non-trivial computer program.</li>
<li>Familiarity with probability theory (CS 109 or STATS 116)</li>
<li>Familiarity with linear algebra (any one of Math 104, Math 113, or CS 205 should be sufficient)</li>
</ul>
</li>
<li>Course Description: This course provides a broad introduction to machine learning and statistical pattern recognition. Topics include: supervised learning (generative/discriminative learning, parametric/non-parametric learning, neural networks, support vector machines); unsupervised learning (clustering, dimensionality reduction, kernel methods); learning theory (bias/variance tradeoffs; VC theory; large margins); reinforcement learning and adaptive control. The course will also discuss recent applications of machine learning, such as to robotic control, data mining, autonomous navigation, bioinformatics, speech recognition, and text and web data processing.</li>
</ul>
</li>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{UG}$</strong>NTU Machine Learning Foundations by <a href="https://www.csie.ntu.edu.tw/~htlin/" target="_blank" rel="external"><strong>Hsuan-Tien Lin&#x6797;&#x8ED2;&#x7530;</strong></a></font>: <a href="https://www.coursera.org/learn/ntumlone-mathematicalfoundations" target="_blank" rel="external">Course Page|Part I Mathematical Foundations</a>, <a href="https://www.coursera.org/learn/ntumlone-algorithmicfoundations" target="_blank" rel="external">Course Page|Part II Algorithmic Foundations</a>, <a href="https://www.youtube.com/playlist?list=PLXVfgk9fNX2I7tB6oIINGBmW50rrmFTqf" target="_blank" rel="external">Videos<i class="fa fa-youtube" aria-hidden="true"></i></a><ul>
<li>Intermediate level</li>
<li>Course Description: Machine learning is the study that allows computers to adaptively improve their performance with experience accumulated from the data observed. Our two sister courses teach the most fundamental algorithmic, theoretical and practical tools that any user of machine learning needs to know. This first course of the two would focus more on mathematical tools, and the other course would focus more on algorithmic tools.</li>
</ul>
</li>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{UG}$</strong>NTU Machine Learning Techniques by <a href="https://www.csie.ntu.edu.tw/~htlin/" target="_blank" rel="external"><strong>Hsuan-Tien Lin&#x6797;&#x8ED2;&#x7530;</strong></a></font>: <a href="http://www.ntumooc.org/single-post/2014/11/05/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%8A%80%E6%B3%95-Machine-Learning-Techniques" target="_blank" rel="external">Course Page</a>, <a href="https://www.youtube.com/playlist?list=PLXVfgk9fNX2IQOYPmqjqWsNUFl2kpk1U2" target="_blank" rel="external">Videos<i class="fa fa-youtube" aria-hidden="true"></i></a><ul>
<li>Prerequisite:<ul>
<li>Machine Learning Foundations by Hsuan-Tien Lin or equivalent</li>
<li>Recommended reading: <a href="http://amlbook.com/" target="_blank" rel="external">Learning from Data</a></li>
</ul>
</li>
<li>This course covers:<ul>
<li>Embedding numerous features: linear support vector machine, dual support vector machine, kernel support vector machine, soft-margin support vector machine, kernel logistic regression, support vector regression</li>
<li>Combining predictive features: blending and bagging, adaptive boosting, decision tree, random forest, gradient boosted decision tree</li>
<li>Distilling hidden features: neural network, deep learning, radial basis function network, matrix factorization</li>
</ul>
</li>
</ul>
</li>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{G}$</strong>Columbia 4772 Advanced Machine Learning by <strong>Tony Jebara</strong>, Spring 2015</font>: <a href="http://www.cs.columbia.edu/~jebara/4772/index.html" target="_blank" rel="external">Course Page</a><ul>
<li>Prerequisites: COMS W4771 Machine Learning or permission. Background in linear algebra and statistics.</li>
<li>Course Description: Advanced topics in machine learning including: Linear Modeling, Nonlinear Dimension Reduction, Maximum Entropy, Exponential Family Models, Conditional Random Fields, Graphical Models, Structured Support Vector Machines, Feature Selection, Kernel Selection, Meta-Learning, Multi-Task Learning, Semi-Supervised Learning, Graph-Based Semi-Supervised Learning, Approximate Inference, Clustering, and Boosting.</li>
</ul>
</li>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{G}$</strong>CMU 10-701/15-781 - Machine Learning by <strong><a href="http://www.cs.cmu.edu/~tom/" target="_blank" rel="external">Tom Mitchell</a></strong>, Spring 2011</font>: 1h15min/Lec, 26 Lectures, <a href="http://www.cs.cmu.edu/~tom/10701_sp11/" target="_blank" rel="external">Course Page</a>, <a href="http://www.cs.cmu.edu/~tom/10701_sp11/lectures.shtml" target="_blank" rel="external">Videos</a><ul>
<li>Prerequisites: Students entering the class are expected to have a pre-existing working knowledge of <strong>probability, linear algebra, statistics and algorithms</strong>, though the class has been designed to allow students with a strong numerate background to catch up and fully participate. In addition, recitation sessions will be held to review some basic concepts.</li>
<li>Course Description: This course covers the theory and practical algorithms for machine learning from a variety of perspectives. We cover topics such as Bayesian networks, decision tree learning, Support Vector Machines, statistical learning methods, unsupervised learning and reinforcement learning. The course covers theoretical concepts such as inductive bias, the PAC learning framework, Bayesian learning methods, margin-based learning, and Occam&#x2019;s Razor. Short programming assignments include hands-on experiments with various learning algorithms, and a larger course project gives students a chance to dig into an area of their choice. This course is designed to give a <strong>graduate-level</strong> student a thorough grounding in the methodologies, technologies, mathematics and algorithms currently needed by people who do research in machine learning.</li>
<li>To choose between the Introduction to Machine Learning courses (10-401, 10-601, 10-701, and 10-715), please read the <strong><a href="https://docs.google.com/document/d/1Y0Jx_tcINWQrWJx31WGEQSsUs059OUMmPIVSeyxNdeM/edit" target="_blank" rel="external">Intro to ML Course Comparison</a></strong>.</li>
</ul>
</li>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{G}$</strong>CMU 10-701/15-781 Introduction to Machine Learning by <a href="https://alex.smola.org/index.html" target="_blank" rel="external"><strong>Alexander J. Smola</strong></a></font>: <a href="http://alex.smola.org/teaching/cmu2013-10-701/" target="_blank" rel="external">Course Page|Fall2013</a>, <a href="https://www.youtube.com/playlist?list=PLZSO_6-bSqHR7NPk4k0zqdm2dPdraQZ_B" target="_blank" rel="external">Videos||Fall2013<i class="fa fa-youtube" aria-hidden="true"></i></a>, <a href="http://alex.smola.org/teaching/10-701-15/" target="_blank" rel="external">Course Page|Spring2015</a>, <a href="https://www.youtube.com/playlist?list=PLZSO_6-bSqHTTV7w9u7grTXBHMH-mw3qn" target="_blank" rel="external">Videos|Spring2015<i class="fa fa-youtube" aria-hidden="true"></i></a><ul>
<li>Prerequisites:<ul>
<li>Basic probability and statistics are a plus.</li>
<li>Basic linear algebra (matrices, vectors, eigenvalues) is a plus. Knowing functional analysis would be great but not required.</li>
<li>Ability to write code that exceeds &#x2018;Hello World&#x2019;. Preferably beyond Matlab or R.</li>
<li>Basic knowledge of optimization. Having attended a convex optimization class would be great but the recitations will cover this.</li>
<li>You should have no trouble answering the questions of the <a href="http://www.cs.cmu.edu/~wcohen/10-601/self-assessment/Intro_ML_Self_Evaluation.pdf" target="_blank" rel="external">self evaluation</a> handed out for the 10-601 Introduction to Machine Learning course.</li>
</ul>
</li>
<li>Course Description: This course is designed to give PhD students a thorough grounding in the methods, theory, mathematics and algorithms needed to do research and applications in machine learning. The topics of the course draw from machine learning, classical statistics, data mining, Bayesian statistics and information theory. Students entering the class with a pre-existing working knowledge of probability, statistics and algorithms will be at an advantage, but the class has been designed so that anyone with a strong numerate background can catch up and fully participate.</li>
<li>To choose between the Introduction to Machine Learning courses (10-401, 10-601, 10-701, and 10-715), please read the <strong><a href="https://docs.google.com/document/d/1Y0Jx_tcINWQrWJx31WGEQSsUs059OUMmPIVSeyxNdeM/edit" target="_blank" rel="external">Intro to ML Course Comparison</a></strong>.</li>
</ul>
</li>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{G}$</strong>CMU 10-701 Introduction to Machine Learning by <a href="http://www.cs.cmu.edu/~epxing/" target="_blank" rel="external"><strong>Eric Xing</strong></a> and <strong><a href="http://www.cs.cmu.edu/~zivbj/" target="_blank" rel="external">Ziv Bar-Joseph</a></strong>, Fall 2015</font>: <a href="http://www.cs.cmu.edu/~epxing/Class/10701/index.html" target="_blank" rel="external">Course Page</a>, <a href="https://mediatech-stream.andrew.cmu.edu/Mediasite/Catalog/Full/b055772df9a34215b7251fded1ba258e21" target="_blank" rel="external">Videos</a><ul>
<li>Course Description: Machine learning studies the question &#x201C;how can we build computer programs that automatically improve their performance through experience?&#x201D; This includes learning to perform many types of tasks based on many types of experience. For example, it includes robots learning to better navigate based on experience gained by roaming their environments, medical decision aids that learn to predict which therapies work best for which diseases based on data mining of historical health records, and speech recognition systems that lean to better understand your speech based on experience listening to you.This course is designed to give PhD students a thorough grounding in the methods, theory, mathematics and algorithms needed to do research and applications in machine learning. The topics of the course draw from from machine learning, from classical statistics, from data mining, from Bayesian statistics and from information theory. Students entering the class with a pre-existing working knowledge of probability, statistics and algorithms will be at an advantage, but the class has been designed so that anyone with a strong numerate background can catch up and fully participate.</li>
<li>To choose between the Introduction to Machine Learning courses (10-401, 10-601, 10-701, and 10-715), please read the <strong><a href="https://docs.google.com/document/d/1Y0Jx_tcINWQrWJx31WGEQSsUs059OUMmPIVSeyxNdeM/edit" target="_blank" rel="external">Intro to ML Course Comparison</a></strong>.</li>
</ul>
</li>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{G}$</strong>CMU 10-715 Advanced Introduction to Machine Learning</font>: <a href="https://www.cs.cmu.edu/~epxing/Class/10715/" target="_blank" rel="external">Eric Xing|2014Fall</a>, <a href="http://www.cs.cmu.edu/~bapoczos/Classes/ML10715_2015Fall/" target="_blank" rel="external">Alex Smola|2015Fall</a>, <a href="https://www.youtube.com/playlist?list=PL4DwY1suLMkcu-wytRDbvBNmx57CdQ2pJ&amp;disable_polymer=true" target="_blank" rel="external">Videos|2015Fall<i class="fa fa-youtube" aria-hidden="true"></i></a><ul>
<li>Prerequisites: Students entering the class are expected to have a pre-existing strong working knowledge of algorithms, linear algebra, probability, and statistics. If you are interested in this topic, but do not have the required background or are not planning to work on a PhD thesis with machine learning as the main focus, you might consider the general graduate Machine Learning course (10-701) or the Masters-level Machine Learning course (10-601).</li>
<li>Course Description: This course is designed for Ph.D. students whose primary field of study is machine learning, or who intend to make machine learning methodological research a main focus of their thesis. It will give students a thorough grounding in the algorithms, mathematics, theories, and insights needed to do in-depth research and applications in machine learning. The topics of this course will in part parallel those covered in the general graduate machine learning course (10-701), but with a greater emphasis on depth in theory and algorithms. The course will also include additional advanced topics such as RKHS and representer theory, Bayesian nonparametrics, additional material on graphical models, manifolds and spectral graph theory, reinforcement learning and online learning, etc.</li>
</ul>
</li>
</ol>
<h3 id="Deep-Learning"><a href="#Deep-Learning" class="headerlink" title="Deep Learning"></a>Deep Learning</h3><ol>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{UG}$</strong><span id="OP3"></span><strong>Coursera - Neural Networks for Machine Learning</strong> by <a href="http://www.cs.toronto.edu/~hinton/" target="_blank" rel="external"><strong>Geoffrey Hinton</strong></a></font>: 16 Lectures, <a href="https://www.coursera.org/learn/neural-networks" target="_blank" rel="external">Course Page 1</a>, <a href="http://www.cs.toronto.edu/~tijmen/csc321/" target="_blank" rel="external">Course Page 2</a>, <a href="https://www.youtube.com/playlist?list=PLoRl3Ht4JOcdU872GhiYWf6jwrk_SNhz9" target="_blank" rel="external">Videos<i class="fa fa-youtube" aria-hidden="true"></i></a> <i class="fa fa-forward" aria-hidden="true"></i> <img src="http://progressed.io/bar/44?title=done" alt="34/78"><ul>
<li>Prerequisites:<ul>
<li>Calculus: (MAT135H1, MAT136H1)/MAT135Y1/MAT137Y1/MAT157Y1</li>
<li>Linear Algebra: MAT223H1/MAT240H1</li>
<li>Statistics and Probability: STA247H1/STA255H1/STA257H1</li>
</ul>
</li>
<li>Course Description: Learn about artificial neural networks and how they&#x2019;re being used for machine learning, as applied to speech and object recognition, image segmentation, modeling language and human motion, etc. We&#x2019;ll emphasize both the basic algorithms and the practical tricks needed to get them to work well. Please be advised that the course is suited for an <strong>intermediate level learner</strong> - comfortable with calculus and with experience programming (Python).</li>
</ul>
</li>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{UG}$</strong>Coursera - Neural Networks and Deep Learning by <strong>Andrew Ng</strong></font>: 46 parts, <a href="https://www.coursera.org/learn/neural-networks-deep-learning" target="_blank" rel="external">Course Page</a>,<a href="https://www.youtube.com/playlist?list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K" target="_blank" rel="external">Videos<i class="fa fa-youtube" aria-hidden="true"></i></a><ul>
<li>Prerequisites:<ul>
<li>Expected:<ul>
<li>Programming: Basic Python programming skills, with the capability to work effectively with data structures.</li>
</ul>
</li>
<li>Recommended:<ul>
<li>Mathematics: Matrix vector operations and notation.</li>
<li>Machine Learning: Understanding how to frame a machine learning problem, including how data is represented will be beneficial. (If you have taken Stanford CS229 - Machine Learning by Andrew Ng, you have much more than the needed level of knowledge.)</li>
</ul>
</li>
</ul>
</li>
<li>Course Description: In this course, you will learn the foundations of deep learning. This course also teaches you how Deep Learning actually works, rather than presenting only a cursory or surface-level description. So after completing it, you will be able to apply deep learning to a your own applications. If you are looking for a job in AI, after this course you will also be able to answer basic interview questions. When you finish this class, you will:<ul>
<li>Understand the major technology trends driving Deep Learning</li>
<li>Be able to build, train and apply fully connected deep neural networks</li>
<li>Know how to implement efficient (vectorized) neural networks</li>
<li>Understand the key parameters in a neural network&#x2019;s architecture</li>
</ul>
</li>
</ul>
</li>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{UG}$</strong><span id="OP2"></span><strong>UCambridge - Course on Information Theory, Pattern Recognition, and Neural Networks</strong> by <strong>David MacKay</strong></font>: 1h30min/Lec, 16 Lectures, <a href="http://www.inference.org.uk/itprnn/" target="_blank" rel="external">Course Page</a>, <a href="http://videolectures.net/course_information_theory_pattern_recognition/" target="_blank" rel="external">Videos-1<i class="fa fa-external-link" aria-hidden="true"></i></a>, <a href="https://www.youtube.com/playlist?list=PLruBu5BI5n4aFpG32iMbdWoRVAA-Vcso6" target="_blank" rel="external">Videos-2<i class="fa fa-youtube" aria-hidden="true"></i></a> <i class="fa fa-forward" aria-hidden="true"></i> <img src="http://progressed.io/bar/32?title=done" alt="5/16"><ul>
<li>Prerequisites: <ul>
<li>Probability and statistics</li>
<li>Linear algebra</li>
<li>Statistical physics</li>
</ul>
</li>
<li>Course Description: This course will cover Introduction to information theory, Entropy and data compression, Communication over noisy channels, Statistical inference, data modelling and pattern recognition, Approximation of probability distributions and Neural networks and content-addressable memories.</li>
</ul>
</li>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{UG}$</strong>Stanford CS231N - Convolutional Neural Networks for Visual Recognition Spring 2017 by <strong>Li Feifei</strong></font>: 1h15min/Lec, 16 Lectures, <a href="http://cs231n.stanford.edu/" target="_blank" rel="external">Course Page</a>,<a href="https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv" target="_blank" rel="external">Videos<i class="fa fa-youtube" aria-hidden="true"></i></a><ul>
<li>Prerequisites:<ul>
<li>Proficiency in Python, high-level familiarity in C/C++</li>
<li>College Calculus, Linear Algebra (e.g. MATH 19 or 41, MATH 51)</li>
<li>Basic Probability and Statistics (e.g. CS 109 or other stats course)</li>
<li>Equivalent knowledge of CS229 (Machine Learning)</li>
</ul>
</li>
<li>Course Description: This course is a deep dive into details of the deep learning architectures with a focus on learning end-to-end models for these tasks, particularly image classification. During the 10-week course, students will learn to implement, train and debug their own neural networks and gain a detailed understanding of cutting-edge research in computer vision. The final assignment will involve training a multi-million parameter convolutional neural network and applying it on the largest image classification dataset (ImageNet). We will focus on teaching how to set up the problem of image recognition, the learning algorithms (e.g. backpropagation), practical engineering tricks for training and fine-tuning the networks and guide the students through hands-on assignments and a final course project. Much of the background and materials of this course will be drawn from the ImageNet Challenge.</li>
</ul>
</li>
</ol>
<h3 id="Natural-Language-Processing"><a href="#Natural-Language-Processing" class="headerlink" title="Natural Language Processing"></a>Natural Language Processing</h3><ol>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{UG}$</strong><span id="OP6"></span><strong>Coursera - Natural Language Processing</strong> by <a href="https://web.stanford.edu/~jurafsky/" target="_blank" rel="external"><strong>Dan Jurafsky</strong></a> &amp; <a href="https://nlp.stanford.edu/manning/" target="_blank" rel="external"><strong>Chris Manning</strong></a></font>: 23 Lectures, <a href="https://class.coursera.org/nlp/" target="_blank" rel="external">Course Page</a>, <a href="http://www.mohamedaly.info/teaching/cmp-462-spring-2013" target="_blank" rel="external">Resources</a>, <a href="https://www.youtube.com/playlist?list=PL6397E4B26D00A269" target="_blank" rel="external">Videos<i class="fa fa-youtube" aria-hidden="true"></i></a> <i class="fa fa-forward" aria-hidden="true"></i> <img src="http://progressed.io/bar/35?title=done" alt="37/102"><ul>
<li>Course Description: This course covers a broad range of topics in natural language processing, including word and sentence tokenization, text classification and sentiment analysis, spelling correction, information extraction, parsing, meaning extraction, and question answering, We will also introduce the underlying theory from probability, statistics, and machine learning that are crucial for the field, and cover fundamental algorithms like n-gram language modeling, naive bayes and maxent classifiers, sequence models like Hidden Markov Models, probabilistic dependency and constituent parsing, and vector-space models of meaning.</li>
</ul>
</li>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{UG}$</strong><span id="OP7"></span><strong>Stanford CS224N - Deep Learning for Natural Language Processing</strong> by <strong>Chris Manning</strong></font>: 1h15min/Lec, 18 Lectures, <a href="http://web.stanford.edu/class/cs224n/" target="_blank" rel="external">Course Page</a>, <a href="https://www.youtube.com/playlist?list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6" target="_blank" rel="external">Videos<i class="fa fa-youtube" aria-hidden="true"></i></a> <i class="fa fa-play" aria-hidden="true"></i> <img src="http://progressed.io/bar/11?title=done" alt="2/18"><ul>
<li>Prerequisites:<ul>
<li>Proficiency in Python</li>
<li>College Calculus, Linear Algebra (e.g. MATH 51, CME 100)</li>
<li>Basic Probability and Statistics (e.g. CS 109 or other stats course)</li>
<li>Foundations of Machine Learning</li>
</ul>
</li>
<li>Course Description: The course provides a thorough introduction to cutting-edge research in deep learning applied to NLP. On the model side we will cover word vector representations, window-based neural networks, recurrent neural networks, long-short-term-memory models, recursive neural networks, convolutional neural networks as well as some recent models involving a memory component. Through lectures and programming assignments students will learn the necessary engineering tricks for making neural networks work on practical problems.</li>
</ul>
</li>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{UG}$</strong>Columbia COMS W4705 - Natural Language Processing by <a href="http://www.cs.columbia.edu/~mcollins/" target="_blank" rel="external"><strong>Michael Collins</strong></a></font>: <a href="http://www.cs.columbia.edu/~cs4705/" target="_blank" rel="external">Course Page</a>, <a href="https://www.youtube.com/playlist?list=PL0ap34RKaADMjqjdSkWolD-W2VSCyRUQC" target="_blank" rel="external">Videos<i class="fa fa-youtube" aria-hidden="true"></i></a><ul>
<li>Course Description: This course covers Introduction to NLP, Language Modeling, Tagging, and Hidden Markov Models, Parsing, and Context-free Grammars, Probabilistic Context-free Grammars, Lexicalized PCFGs, The IBM Translation Models, Phrase-Based Translation Models, Log-Linear Models and Log-Linear Models for Tagging, and Global Linear Models, Global Linear Models for Tagging and for dependency parsing.</li>
</ul>
</li>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{G}$</strong>MIT 6.864 Advanced Natural Language Processing</font>: <a href="http://people.csail.mit.edu/regina/6864/" target="_blank" rel="external">Course Page Fall 2010</a>, <a href="http://people.csail.mit.edu/regina/6864-12/index.html" target="_blank" rel="external">Course Page Fall 2012</a>, <a href="http://courses.csail.mit.edu/6.864/" target="_blank" rel="external">Course Page Fall 2016</a><ul>
<li>Course Description: The need to study human languages from a computational perspective has never been greater. Much of the vast amounts of information available today is in a textual form, requiring us to develop automated tools to search, extract, translate, and summarize the data. This course on natural language processing (NLP) focuses exactly on such problems, covering syntactic, semantic and discourse processing models, and their applications to information extraction, machine translation, and text summarization. As a new feature this year, the course will emphasize deep learning techniques for NLP, introducing them in parallel and comparatively with more traditional approaches to NLP.</li>
</ul>
</li>
</ol>
<h3 id="Probabilistic-Graphical-Models"><a href="#Probabilistic-Graphical-Models" class="headerlink" title="Probabilistic Graphical Models"></a>Probabilistic Graphical Models</h3><ol>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{UG}$</strong>/<strong>$\mathbb{G}$</strong><span id="OP4"></span>Coursera - Probabilistic Graphical Models by <a href="http://ai.stanford.edu/users/koller/" target="_blank" rel="external"><strong>Daphne Koller</strong></a></font>: 15 Lectures, <a href="https://www.coursera.org/learn/probabilistic-graphical-models/home/welcome" target="_blank" rel="external">Course Page 1 - Representation</a>, <a href="https://www.coursera.org/learn/probabilistic-graphical-models-2-inference/home/welcome" target="_blank" rel="external">Course Page 2 - Inference</a>, <a href="https://www.coursera.org/learn/probabilistic-graphical-models-3-learning/home/welcome" target="_blank" rel="external">Course Page 3 - Learning</a>, <a href="https://mitpress.mit.edu/books/probabilistic-graphical-models" target="_blank" rel="external">Textbook</a>, <a href="https://www.youtube.com/playlist?list=PL50E6E80E8525B59C" target="_blank" rel="external">Videos<i class="fa fa-youtube" aria-hidden="true"></i></a> <i class="fa fa-play" aria-hidden="true"></i> <img src="http://progressed.io/bar/37?title=done" alt="35/94"><ul>
<li>Prerequisite: Familiarity with programming, basic linear algebra (matrices, vectors, matrix-vector multiplication), and basic probability (random variables, basic properties of probability) is assumed. Basic calculus (derivatives and partial derivatives) would be helpful and would give you additional intuitions about the algorithms, but isn&#x2019;t required to fully complete this course.</li>
<li>Course Description:<ul>
<li>Part 1 Inference: This course describes the two basic PGM representations: Bayesian Networks, which rely on a directed graph; and Markov networks, which use an undirected graph. The course discusses both the theoretical properties of these representations as well as their use in practice. The (highly recommended) honors track contains several hands-on assignments on how to represent some real-world problems. The course also presents some important extensions beyond the basic PGM representation, which allow more complex models to be encoded compactly.</li>
<li>Part 2 Inference: This course addresses the question of probabilistic inference: how a PGM can be used to answer questions. Even though a PGM generally describes a very high dimensional distribution, its structure is designed so as to allow questions to be answered efficiently. The course presents both exact and approximate algorithms for different types of inference tasks, and discusses where each could best be applied. The (highly recommended) honors track contains two hands-on programming assignments, in which key routines of the most commonly used exact and approximate algorithms are implemented and applied to a real-world problem.</li>
<li>Part 3 Learning: This course addresses the question of learning: how a PGM can be learned from a data set of examples. The course discusses the key problems of parameter estimation in both directed and undirected models, as well as the structure learning task for directed models. The (highly recommended) honors track contains two hands-on programming assignments, in which key routines of two commonly used learning algorithms are implemented and applied to a real-world problem.</li>
</ul>
</li>
</ul>
</li>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{G}$</strong>CMU 10-708 - Probabilistic Graphical Models by <strong><a href="http://www.cs.cmu.edu/~epxing/" target="_blank" rel="external">Eric Xing</a></strong>, Spring 2014</font>: <a href="http://www.cs.cmu.edu/~epxing/Class/10708/index.html" target="_blank" rel="external">Course Page</a>, <a href="https://www.youtube.com/playlist?list=PLI3nIOD-p5aoXrOzTd1P6CcLavu9rNtC-" target="_blank" rel="external">Videos<i class="fa fa-youtube" aria-hidden="true"></i></a><ul>
<li>Prerequisites: In order to take this class, students are required to have successfully completed Machine Learning 10-701/15-781, or an equivalent class.</li>
<li>Course Description:<ul>
<li>Many of the problems in artificial intelligence, statistics, computer systems, computer vision, natural language processing, and computational biology, among many other fields, can be viewed as the search for a coherent global conclusion from local information. The probabilistic graphical models framework provides an unified view for this wide range of problems, enabling efficient inference, decision-making and learning in problems with a very large number of attributes and huge datasets. This graduate-level course will provide you with a strong foundation for both applying graphical models to complex problems and for addressing core research topics in graphical models.</li>
<li>The class will cover three aspects: the core representation, including Bayesian and Markov networks, and dynamic Bayesian networks; probabilistic inference algorithms, both exact and approximate; and learning methods for both the parameters and the structure of graphical models. Students entering the class should have a pre-existing working knowledge of probability, statistics, and algorithms, though the class has been designed to allow students with a strong mathematical background to catch up and fully participate.</li>
<li>It is expected that after taking this class, students will have obtained sufficient working knowledge of multi-variate probablistic modeling and inference for practical applications, should be able to fomulate and solve a wide range of problems in their own domain using graphical models, and can advance into more specialized technical literature by themselves.</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h3><ol>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{UG}$</strong><span id="OP5"></span>Udacity|Georgia Tech CS7646 - Machine Learning for Trading by <strong>Tucker Balch</strong></font>: 28 parts, <a href="https://www.udacity.com/course/machine-learning-for-trading--ud501" target="_blank" rel="external">Course Page</a>, <a href="https://www.youtube.com/playlist?list=PLAwxTw4SYaPnIRwl6rad_mYwEk4Gmj7Mx" target="_blank" rel="external">Videos<i class="fa fa-youtube" aria-hidden="true"></i></a> <img src="http://progressed.io/bar/4?title=done" alt="1/28"><ul>
<li>Course Description: This course introduces students to the real world challenges of implementing machine learning based trading strategies including the algorithmic steps from information gathering to market orders. The focus is on how to apply probabilistic machine learning approaches to trading decisions. We consider statistical approaches like linear regression, KNN and regression trees and how to apply them to actual stock trading situations.</li>
</ul>
</li>
</ol>
<h3 id="Other-courses"><a href="#Other-courses" class="headerlink" title="Other courses"></a>Other courses</h3><ol>
<li><strong>$\mathbb{UG}$</strong>Berkeley CS 294 Practical Machine Learning by <a href="http://people.eecs.berkeley.edu/~jordan/" target="_blank" rel="external"><strong>Michael I. Jordan</strong></a>, Fall 2009: <a href="https://people.eecs.berkeley.edu/~jordan/courses/294-fall09/" target="_blank" rel="external">Course Page</a><ul>
<li>Prerequisites: some prior exposure to probability and to linear algebra.</li>
<li>Course Description: This course introduces core statistical machine learning algorithms in a (relatively) non-mathematical way, emphasizing applied problem-solving. The prerequisites are light; some prior exposure to basic probability and to linear algebra will suffice.</li>
</ul>
</li>
<li><strong>$\mathbb{UG}$</strong>Harvard CS 109 - Data Science by <a href="http://www.people.fas.harvard.edu/~blitz/Site/Home.html" target="_blank" rel="external">Joe Blitzstein</a>: 1h15min/Lec, 20 Lectures, <a href="http://cs109.github.io/2014/index.html" target="_blank" rel="external">Course Page|2014</a>, <a href="http://cs109.github.io/2015/" target="_blank" rel="external">Course Page|2015</a>, <a href="https://www.youtube.com/playlist?list=PLb4G5axmLqiuneCqlJD2bYFkBwHuOzKus" target="_blank" rel="external">Videos|2014<i class="fa fa-youtube" aria-hidden="true"></i></a>, <a href="https://matterhorn.dce.harvard.edu/engage/ui/index.html#/2016/01/14328?typeNum=L01" target="_blank" rel="external">Videos|2015</a>, <a href="https://github.com/cs109" target="_blank" rel="external">Official Resources<i class="fa fa-github" aria-hidden="true"></i></a><ul>
<li>Prerequisites: Both undergraduates and graduate students are welcome to take the course. You should have been equipped with:<ul>
<li>programming knowledge at the level of CS 50 (or above)</li>
<li>statistics knowledge at the level of Stat 100 (or above).</li>
</ul>
</li>
<li>Course Description: Learning from data in order to gain useful predictions and insights. This course introduces methods for five key facets of an investigation: data wrangling, cleaning, and sampling to get a suitable data set; data management to be able to access big data quickly and reliably; exploratory data analysis to generate hypotheses and intuition; prediction based on statistical methods such as regression and classification; and communication of results through visualization, stories, and interpretable summaries. We will be using Python for all programming assignments and projects.</li>
</ul>
</li>
<li><strong>$\mathbb{UG}$</strong>CMU 10-601 - Machine Learning by <strong><a href="http://www.cs.cmu.edu/~tom/" target="_blank" rel="external">Tom Mitchell</a></strong>, Spring 2015: <a href="https://www.cs.cmu.edu/~ninamf/courses/601sp15/index.html" target="_blank" rel="external">Course Page</a>, <a href="https://www.cs.cmu.edu/~ninamf/courses/601sp15/video/1.html" target="_blank" rel="external">Videos|CMU</a>, <a href="https://www.youtube.com/playlist?list=PLl-BBnDxtUt1hLXmIwu27P22bTi6VwMkN" target="_blank" rel="external">Videos<i class="fa fa-youtube" aria-hidden="true"></i></a><ul>
<li>Prerequisites: Students entering the class are expected to have a pre-existing working knowledge of probability, linear algebra, statistics and algorithms, though the class has been designed to allow students with a strong numerate background to catch up and fully participate. In addition, recitation sessions will be held to review some basic concepts.</li>
<li>Course Description: This course covers the theory and practical algorithms for machine learning from a variety of perspectives. We cover topics such as Bayesian networks, decision tree learning, Support Vector Machines, statistical learning methods, unsupervised learning and reinforcement learning. The course covers theoretical concepts such as inductive bias, the PAC learning framework, Bayesian learning methods, margin-based learning, and Occam&#x2019;s Razor. Short programming assignments include hands-on experiments with various learning algorithms. This course is designed to give a graduate-level student a thorough grounding in the methodologies, technologies, mathematics and algorithms currently needed by people who do research in machine learning.</li>
<li>To choose between the Introduction to Machine Learning courses (10-401, 10-601, 10-701, and 10-715), please read the <strong><a href="https://docs.google.com/document/d/1Y0Jx_tcINWQrWJx31WGEQSsUs059OUMmPIVSeyxNdeM/edit" target="_blank" rel="external">Intro to ML Course Comparison</a></strong>.</li>
<li><a href="https://www.quora.com/Which-is-better-at-CMU-10-601-or-10-701" target="_blank" rel="external">Quora|Which is better at CMU: 10-601 or 10-701?</a></li>
</ul>
</li>
<li><strong>$\mathbb{G}$</strong> CMU 10-707 Topics in Deep Learning by <strong>Russ Salakhutdinov</strong>, Fall 2017: <a href="http://www.cs.cmu.edu/~rsalakhu/10707/" target="_blank" rel="external">Course Page</a>, <a href="https://www.youtube.com/playlist?list=PLpIxOj-HnDsOSL__Buy7_UEVQkyfhHapa" target="_blank" rel="external">Videos<i class="fa fa-youtube" aria-hidden="true"></i></a><ul>
<li>Prerequisites: This is an advanced <strong>graduate course</strong>, designed for Masters and Ph.D. level students, and will assume a reasonable degree of mathematical maturity:<ul>
<li>ML: 10-701 or 10-715, and strong programming skills.</li>
</ul>
</li>
<li>Course Description: The goal of this course is to introduce students to the recent and exciting developments of various deep learning methods. Some topics to be covered include: restricted Boltzmann machines (RBMs) and their multi-layer extensions Deep Belief Networks and Deep Boltzmann machines; sparse coding, autoencoders, variational autoencoders, convolutional neural networks, recurrent neural networks, generative adversarial networks, and attention-based models with applications in vision, NLP, and multimodal learning. We will also address mathematical issues, focusing on efficient large-scale optimization methods for inference and learning, as well as training density models with intractable partition functions.</li>
</ul>
</li>
<li><strong>$\mathbb{G}$</strong>UBC CPSC540 Machine Learning by <a href="https://www.cs.ox.ac.uk/people/nando.defreitas/" target="_blank" rel="external"><strong>Nando de Freitas</strong></a>: 21 Lectures, <a href="http://www.cs.ubc.ca/~nando/540-2013/lectures.html" target="_blank" rel="external">Course Page</a>, <a href="https://www.youtube.com/playlist?list=PLE6Wd9FR--EdyJ5lbFl8UuGjecvVw66F6&amp;feature=view_all" target="_blank" rel="external">Videos<i class="fa fa-youtube" aria-hidden="true"></i></a></li>
<li>Oxford Deep learning by <strong>Nando de Freitas</strong>, 2015: <a href="http://www.cs.ox.ac.uk/teaching/courses/2014-2015/ml/" target="_blank" rel="external">Course Page</a>, <a href="https://www.youtube.com/playlist?list=PLE6Wd9FR--EfW8dtjAuPoTuPcqmOV53Fu" target="_blank" rel="external">Videos<i class="fa fa-youtube" aria-hidden="true"></i></a></li>
<li><strong>$\mathbb{UG}$</strong>Machine Learning by @mathematicalmonk (<a href="http://www2.stat.duke.edu/~jwm40/teaching.html" target="_blank" rel="external">Jeff Miller</a> on Duke): 160 videos, 909k+ views, <a href="https://www.youtube.com/playlist?list=PLD0F06AA0D2E8FFBA" target="_blank" rel="external">Videos<i class="fa fa-youtube" aria-hidden="true"></i></a></li>
<li>WUSTL CSE 515T Bayesian Methods in Machine Learning by <strong>Roman Garnett</strong>, Spring 2015: <a href="http://www.cse.wustl.edu/~garnett/cse515t/spring_2015/" target="_blank" rel="external">Course Page</a></li>
<li><strong>$\mathbb{G}$</strong>MIT 6.867 - Machine Learning by <strong>Tommi Jaakkola</strong>: <a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-867-machine-learning-fall-2006/index.htm" target="_blank" rel="external">Course Page</a><ul>
<li>Course Description: 6.867 is an introductory course on machine learning which gives an overview of many concepts, techniques, and algorithms in machine learning, beginning with topics such as classification and linear regression and ending up with more recent topics such as boosting, support vector machines, hidden Markov models, and Bayesian networks. The course will give the student the basic ideas and intuition behind modern machine learning methods as well as a bit more formal understanding of how, why, and when they work. The underlying theme in the course is statistical inference as it provides the foundation for most of the methods covered.</li>
</ul>
</li>
<li><strong>$\mathbb{UG}$</strong>MIT 6.803/6.833 The Human Intelligence Enterprise: <a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-803-the-human-intelligence-enterprise-spring-2002/" target="_blank" rel="external">Course Page 2002</a>,<a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-803-the-human-intelligence-enterprise-spring-2006/" target="_blank" rel="external">Course Page 2006</a>,<a href="https://courses.csail.mit.edu/6.803/" target="_blank" rel="external">Course Page 2017</a></li>
<li><strong>$\mathbb{G}$</strong>MIT Probabilistic Graphical Models by <strong>David Sontag</strong>, Spring 2013: <a href="http://people.csail.mit.edu/dsontag/courses/pgm13/" target="_blank" rel="external">Course Page</a></li>
<li><strong>$\mathbb{G}$</strong>Stanford CS224U - Natural Language Understanding by <strong>Bill MacCartney</strong>: <a href="http://web.stanford.edu/class/cs224u/" target="_blank" rel="external">Course Page</a></li>
<li><strong>$\mathbb{G}$</strong>Stanford CS224M : Multi Agent Systems by <strong>Yoav Shoham</strong>: <a href="http://web.stanford.edu/class/cs224m/" target="_blank" rel="external">Course Page</a>, <a href="https://lagunita.stanford.edu/courses/Engineering/CS224M/Spring2014/about" target="_blank" rel="external">Videos</a></li>
<li><strong>$\mathbb{G}$</strong>Harvard CS281: Advanced Machine Learning by <strong>Ryan Adams</strong>, Fall 2013: <a href="https://www.seas.harvard.edu/courses/cs281/" target="_blank" rel="external">Course Page</a></li>
<li><strong>$\mathbb{G}$</strong>Stanford CS 228 Probabilistic Graphical Models by <strong>Stefano Ermon</strong>, Winter 2016: <a href="http://cs.stanford.edu/~ermon/cs228/index.html" target="_blank" rel="external">Course Page</a></li>
<li><strong>$\mathbb{G}$</strong>MIT 18.657 - Mathematics of Machine Learning by Philippe Rigollet, Fall 2015: <a href="https://ocw.mit.edu/courses/mathematics/18-657-mathematics-of-machine-learning-fall-2015/" target="_blank" rel="external">Course Page</a><ul>
<li>P. Rigollet has another course MIT 18.650.</li>
</ul>
</li>
<li><strong>$\mathbb{G}$</strong>MIT 18.S997 High-Dimensional Statistics by Philippe Rigollet, Spring 2015: <a href="https://ocw.mit.edu/courses/mathematics/18-s997-high-dimensional-statistics-spring-2015/" target="_blank" rel="external">Course Page</a></li>
<li><strong>$\mathbb{G}$</strong>MIT DS-GA-1003/CSCI-GA.2567 - Machine Learning and Computational Statistics by David Sontag, Spring 2014: <a href="http://people.csail.mit.edu/dsontag/courses/ml14/" target="_blank" rel="external">Course Page</a></li>
<li>Willamette University CS449 - Neural Networks: <a href="https://www.willamette.edu/~gorr/classes/cs449/intro.html" target="_blank" rel="external">Course Page</a>, No Videos found</li>
<li>MLSS 2013 T&#xFC;bingen: Graphical Models by <a href="https://www.microsoft.com/en-us/research/people/cmbishop/" target="_blank" rel="external"><strong>Christopher Bishop</strong></a>: <a href="https://www.youtube.com/playlist?list=PL_gHv3O9IlOd_GkUxgnuHcBihEbTjNoUF" target="_blank" rel="external">Videos<i class="fa fa-youtube" aria-hidden="true"></i></a></li>
<li>&#x9F99;&#x661F;&#x8BA1;&#x5212; &#x6E05;&#x534E;&#x5927;&#x5B66;-&#x673A;&#x5668;&#x5B66;&#x4E60; by <a href="http://www.dbs.ifi.lmu.de/~yu_k/" target="_blank" rel="external">YU Kai&#x4F59;&#x51EF;</a> and <a href="http://tongzhang-ml.org/" target="_blank" rel="external">Zhang Tong&#x5F20;&#x6F7C;</a>: 50min/Lec, 19 Lectures, <a href="https://www.bilibili.com/video/av9016433/" target="_blank" rel="external">Videos</a></li>
<li>Coursera|Johns Hopkins University - Data Science Specialization: 10 courses, <a href="https://www.coursera.org/specializations/jhu-data-science" target="_blank" rel="external">Home Page<i class="fa fa-home" aria-hidden="true"></i></a><ul>
<li><strong>Beginner</strong> Specialization. No prior experience required.</li>
<li>This Specialization covers the concepts and tools you&#x2019;ll need throughout the entire data science pipeline, from asking the right kinds of questions to making inferences and publishing results. In the final Capstone Project, you&#x2019;ll apply the skills learned by building a data product using real-world data. At completion, students will have a portfolio demonstrating their mastery of the material.</li>
</ul>
<ol>
<li>The Data Scientist&#x2019;s Toolbox</li>
<li>R Programming</li>
<li>Getting and Cleaning Data</li>
<li>Exploratory Data Analysis</li>
<li>Reproducible Research</li>
<li>Statistical Inference</li>
<li>Regression Models</li>
<li>Practical Machine Learning</li>
<li>Developing Data Products</li>
<li>Data Science Capstone</li>
</ol>
</li>
<li>Coursera|Michigan University - Applied Data Science with Python Specialization: 5 courses, <a href="https://www.coursera.org/specializations/data-science-python" target="_blank" rel="external">Home Page<i class="fa fa-home" aria-hidden="true"></i></a><ul>
<li><strong>Intermediate</strong> Specialization. Some related experience required.</li>
<li>The 5 courses in this University of Michigan specialization introduce learners to data science through the python programming language. This skills-based specialization is intended for learners who have basic a python or programming background, and want to apply statistical, machine learning, information visualization, text analysis, and social network analysis techniques through popular python toolkits such as pandas, matplotlib, scikit-learn, nltk, and networkx to gain insight into their data.</li>
<li>Introduction to Data Science in Python (course 1), Applied Plotting, Charting &amp; Data Representation in Python (course 2), and Applied Machine Learning in Python (course 3) should be taken in order and prior to any other course in the specialization. After completing those, courses 4 and 5 can be taken in any order. All 5 are required to earn a certificate.</li>
</ul>
<ol>
<li>Introduction to Data Science in Python</li>
<li>Applied Plotting, Charting &amp; Data Representation in Python</li>
<li>Applied Machine Learning in Python</li>
<li>Applied Text Mining in Python</li>
<li>Applied Social Network Analysis in Python</li>
</ol>
</li>
<li>Machine Learning with Python by @sentdex: 72 videos 1,817,742 views, <a href="https://www.youtube.com/playlist?list=PLQVvvaa0QuDfKTOs3Keq_kaG2P55YRn5v" target="_blank" rel="external">Videos<i class="fa fa-youtube" aria-hidden="true"></i></a></li>
</ol>
<h2 id="Books-1"><a href="#Books-1" class="headerlink" title="Books"></a>Books</h2><h3 id="Introduction-to-Artificial-Intelligence"><a href="#Introduction-to-Artificial-Intelligence" class="headerlink" title="Introduction to Artificial Intelligence"></a>Introduction to Artificial Intelligence</h3><ul>
<li>Artificial Intelligence: A Modern Approach by S. Ruseull, P. Norvig: <a href="https://www.amazon.com/Artificial-Intelligence-Modern-Approach-3rd/dp/0136042597/ref=pd_lpo_sbs_14_t_0?_encoding=UTF8&amp;psc=1&amp;refRID=3DJMN86JH7FJM8H9XE17" target="_blank" rel="external">Amazon<i class="fa fa-amazon" aria-hidden="true"></i></a>, <a href="http://aima.cs.berkeley.edu/" target="_blank" rel="external">Book Page<i class="fa fa-home" aria-hidden="true"></i></a></li>
</ul>
<h3 id="Machine-Learning-2"><a href="#Machine-Learning-2" class="headerlink" title="Machine Learning"></a>Machine Learning</h3><ul>
<li><strong>Pattern Recognition and Machine Learning by <a href="https://www.microsoft.com/en-us/research/people/cmbishop/" target="_blank" rel="external">Christopher M. Bishop</a>: <a href="https://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738" target="_blank" rel="external">Amazon<i class="fa fa-amazon" aria-hidden="true"></i></a>, <a href="http://www.springer.com/gp/book/9780387310732" target="_blank" rel="external">Book Page@Springer<i class="fa fa-home" aria-hidden="true"></i></a>, <a href="http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf" target="_blank" rel="external">E-Book<i class="fa fa-book" aria-hidden="true"></i></a></strong></li>
<li>Machine Learning: a Probabilistic Perspective by Kevin P. Murphy: <a href="https://www.amazon.com/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020" target="_blank" rel="external">Amazon<i class="fa fa-amazon" aria-hidden="true"></i></a>, <a href="https://www.cs.ubc.ca/~murphyk/MLbook/" target="_blank" rel="external">Book Page<i class="fa fa-book" aria-hidden="true"></i></a>, <a href="https://github.com/probml/pmtk3" target="_blank" rel="external">MLAPP Code<i class="fa fa-github" aria-hidden="true"></i></a></li>
<li>Machine Learning in Python by Michael Bowles: <a href="https://www.amazon.com/Machine-Learning-Python-Techniques-Predictive/dp/1118961749" target="_blank" rel="external">Amazon<i class="fa fa-amazon" aria-hidden="true"></i></a>, <a href="https://pythonizame.s3.amazonaws.com/media/Book/machine-learning-python-essential-techniques-predictive-analysis/file/008c0aac-9784-11e5-964d-04015fb6ba01.pdf" target="_blank" rel="external">E-Book<i class="fa fa-book" aria-hidden="true"></i></a></li>
<li>&#x673A;&#x5668;&#x5B66;&#x4E60; by &#x5468;&#x5FD7;&#x534E; (A.K.A. <code>&#x897F;&#x74DC;&#x4E66;</code>): <a href="https://www.amazon.cn/%E5%9B%BE%E4%B9%A6/dp/B01ARKEV1G" target="_blank" rel="external">Amazon<i class="fa fa-amazon" aria-hidden="true"></i></a>, <a href="https://book.douban.com/subject/26708119/" target="_blank" rel="external">&#x8C46;&#x74E3;&#x8BFB;&#x4E66;</a>, <a href="http://blog.csdn.net/icefire_tyh/article/details/52064910" target="_blank" rel="external">&#x53C2;&#x8003;&#x7B54;&#x6848; &#x603B;&#x76EE;&#x5F55;</a></li>
<li>&#x6A5F;&#x5668;&#x5B78;&#x7FD2;&#xFF1A;&#x4F7F;&#x7528;Python by htygithub: <a href="https://www.gitbook.com/book/htygithub/machine-learning-python/details" target="_blank" rel="external">GitBook<i class="fa fa-book" aria-hidden="true"></i></a>, <a href="https://github.com/htygithub/machine-learning-python" target="_blank" rel="external">htygithub/machine-learning-python<i class="fa fa-github" aria-hidden="true"></i></a></li>
</ul>
<h3 id="Deep-Learning-1"><a href="#Deep-Learning-1" class="headerlink" title="Deep Learning:"></a>Deep Learning:</h3><ul>
<li>Deep Learning by I. Goodfellow, Y. Bengio, A. Courville: <a href="https://www.amazon.com/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1509885549&amp;sr=1-1&amp;keywords=Deep+Learning" target="_blank" rel="external">Amazon<i class="fa fa-amazon" aria-hidden="true"></i></a>,<a href="http://www.deeplearningbook.org/" target="_blank" rel="external">Home Page<i class="fa fa-home" aria-hidden="true"></i></a></li>
<li><a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials" target="_blank" rel="external">Hvass-Labs/TensorFlow-Tutorials<i class="fa fa-github" aria-hidden="true"></i></a></li>
</ul>
<h3 id="Natural-Language-Processing-1"><a href="#Natural-Language-Processing-1" class="headerlink" title="Natural Language Processing"></a>Natural Language Processing</h3><ul>
<li>Foundations of Statistical Natural Language Processing: <a href="https://www.amazon.com/Foundations-Statistical-Natural-Language-Processing/dp/0262133601" target="_blank" rel="external">Amazon<i class="fa fa-amazon" aria-hidden="true"></i></a></li>
<li>&#x7EDF;&#x8BA1;&#x81EA;&#x7136;&#x8BED;&#x8A00;&#x5904;&#x7406;&#xFF08;&#x7B2C;2&#x7248;&#xFF09;by &#x5B97;&#x6210;&#x5E86;: <a href="https://www.amazon.cn/dp/B00EYSQLFM" target="_blank" rel="external">Amazon<i class="fa fa-amazon" aria-hidden="true"></i></a>, <a href="https://book.douban.com/subject/25746399/" target="_blank" rel="external">&#x8C46;&#x74E3;&#x8BFB;&#x4E66;</a></li>
</ul>
<h3 id="Information-Retrieval"><a href="#Information-Retrieval" class="headerlink" title="Information Retrieval"></a>Information Retrieval</h3><ul>
<li>Introduction to Information Retrieval by C. Manning, P. Raghavan, H. Sch&#xFC;tze: <a href="https://www.amazon.com/Introduction-Information-Retrieval-Christopher-Manning/dp/0521865719/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1509885503&amp;sr=1-1&amp;keywords=Introduction+to+Information+Retrieval" target="_blank" rel="external">Amazon<i class="fa fa-amazon" aria-hidden="true"></i></a>, <a href="https://nlp.stanford.edu/IR-book/" target="_blank" rel="external">Book Page<i class="fa fa-home" aria-hidden="true"></i></a></li>
</ul>
<h3 id="Recommender-System"><a href="#Recommender-System" class="headerlink" title="Recommender System"></a>Recommender System</h3><ul>
<li>Recommendation System Handbook: <a href="http://www.cs.ubbcluj.ro/~gabis/DocDiplome/SistemeDeRecomandare/Recommender_systems_handbook.pdf" target="_blank" rel="external">Book Page<i class="fa fa-home" aria-hidden="true"></i></a></li>
<li>&#x63A8;&#x8350;&#x7CFB;&#x7EDF;&#x5B9E;&#x8DF5; by &#x9879;&#x4EAE;: <a href="https://www.amazon.cn/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%B7%B5-%E9%A1%B9%E4%BA%AE/dp/B008AK5YJO/ref=sr_1_3?ie=UTF8&amp;qid=1509885747&amp;sr=8-3&amp;keywords=%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F" target="_blank" rel="external">Amazon<i class="fa fa-amazon" aria-hidden="true"></i></a>, <a href="https://book.douban.com/subject/10769749/" target="_blank" rel="external">&#x8C46;&#x74E3;&#x8BFB;&#x4E66;</a></li>
</ul>
<h3 id="Others-1"><a href="#Others-1" class="headerlink" title="Others"></a>Others</h3><ul>
<li>Data Mining: Practical Machine Learning Tools and Techniques by Witten and Frank: <a href="https://www.cs.waikato.ac.nz/~ml/weka/book.html" target="_blank" rel="external">Book Page<i class="fa fa-home" aria-hidden="true"></i></a></li>
<li>Tutorial Slides by Andrew Moore: <a href="https://www.autonlab.org/tutorials" target="_blank" rel="external">Home Page<i class="fa fa-home" aria-hidden="true"></i></a></li>
<li>&#x7EC8;&#x6781;&#x7B97;&#x6CD5;: <a href="https://book.douban.com/subject/26931905/" target="_blank" rel="external">&#x8C46;&#x74E3;&#x8BFB;&#x4E66;</a></li>
<li>Python for Data Analysis Book by Wes McKinney: <a href="https://www.amazon.com/gp/product/1491957662/ref=as_li_tl?ie=UTF8&amp;tag=quantpytho-20&amp;camp=1789&amp;creative=9325&amp;linkCode=as2&amp;creativeASIN=1491957662&amp;linkId=8c3bf87b221dbcd8f541f0db20d4da83" target="_blank" rel="external">Amazon<i class="fa fa-amazon" aria-hidden="true"></i></a>, <a href="http://wesmckinney.com/pages/book.html" target="_blank" rel="external">Book Page<i class="fa fa-home" aria-hidden="true"></i></a>, <a href="https://github.com/wesm/pydata-book" target="_blank" rel="external">wesm/pydata-book<i class="fa fa-github" aria-hidden="true"></i></a></li>
</ul>
<h2 id="List-of-list"><a href="#List-of-list" class="headerlink" title="List of list"></a>List of list</h2><ul>
<li><a href="http://www.ml.cmu.edu/academics/ml-phd.html" target="_blank" rel="external">CMU PhD Program in Machine Learning</a></li>
<li><a href="http://fastml.com/machine-learning-courses-online/" target="_blank" rel="external">FastML: Machine Learning made easy</a></li>
<li><a href="http://videolectures.net/Top/Computer_Science/Machine_Learning/" target="_blank" rel="external">Machine Learning Category on VideoLectures.Net</a></li>
<li><a href="http://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/" target="_blank" rel="external">A Tour of Machine Learning Algorithms</a></li>
<li><a href="http://ml.memect.com/article/machine-learning-guide.html" target="_blank" rel="external">&#x673A;&#x5668;&#x5B66;&#x4E60;&#x5165;&#x95E8;&#x8D44;&#x6E90;&#x4E0D;&#x5B8C;&#x5168;&#x6C47;&#x603B;&#xFF08;tang_Kaka_back@&#x65B0;&#x6D6A;&#x5FAE;&#x535A;&#xFF09;</a></li>
<li><a href="https://github.com/ty4z2008/Qix/blob/master/dl.md#&#x673A;&#x5668;&#x5B66;&#x4E60;machine-learning&#x6DF1;&#x5EA6;&#x5B66;&#x4E60;deep-learning&#x8D44;&#x6599;chapter-1" target="_blank" rel="external">&#x673A;&#x5668;&#x5B66;&#x4E60;(Machine Learning)&amp;&#x6DF1;&#x5EA6;&#x5B66;&#x4E60;(Deep Learning)&#x8D44;&#x6599;(Chapter 1)<i class="fa fa-github" aria-hidden="true"></i></a></li>
<li><a href="https://github.com/ty4z2008/Qix/blob/master/dl2.md" target="_blank" rel="external">&#x673A;&#x5668;&#x5B66;&#x4E60;(Machine Learning)&amp;&#x6DF1;&#x5EA6;&#x5B66;&#x4E60;(Deep Learning)&#x8D44;&#x6599;(Chapter 2)<i class="fa fa-github" aria-hidden="true"></i></a></li>
<li><a href="http://qianjiye.de/2014/11/machine-learning-resources/" target="_blank" rel="external">qianjiye|&#x673A;&#x5668;&#x5B66;&#x4E60;&#x8D44;&#x6E90;</a></li>
</ul>
<h2 id="Notes-Blog-Talks-and-so-on"><a href="#Notes-Blog-Talks-and-so-on" class="headerlink" title="Notes, Blog, Talks and so on"></a>Notes, Blog, Talks and so on</h2><ul>
<li><a href="https://www.microsoft.com/en-us/research/group/social-computing-beijing/" target="_blank" rel="external">Microsoft-Social Computing (Asia)</a></li>
<li><a href="http://www.cs.toronto.edu/~hinton/nntut.html" target="_blank" rel="external">Geoffrey E. Hinton&#x2019;s Neural Network Tutorials</a></li>
<li><a href="https://people.eecs.berkeley.edu/~jordan/courses.html" target="_blank" rel="external">Michael I. Jordan&#x2019;s Courses</a></li>
<li><a href="http://dicook.org/index.html" target="_blank" rel="external">Professor Di Cook, Department of Econometrics and Business Statistics, Monash Uni</a><ul>
<li>Professor Di Cook : I am a Fellow of the American Statistical Association, and Ordinary Member of the R Foundation. My research is in data visualisation, exploratory data analysis, multivariate methods, data mining and statistical computing. I have developed methods for visualising high-dimensional data using tours, projection pursuit, manual controls for tours, pipelines for interactive graphics, a grammar of graphics for biological data, and visualizing boundaries in high-d classifiers&#x2026;</li>
</ul>
</li>
<li><a href="https://www.r-bloggers.com/" target="_blank" rel="external">R-blogger</a><ul>
<li>R news and tutorials contributed by (580) R bloggers</li>
</ul>
</li>
<li><a href="https://prdeepakbabu.wordpress.com/" target="_blank" rel="external">Data Science Greedy</a><ul>
<li>A passionate data scientist, machine learning professional, business analytics guy, visualization, big data, Data management, research professional with a passion for data and its application to problem solving.</li>
</ul>
</li>
<li><a href="http://public.econ.duke.edu/~ap172/code.html" target="_blank" rel="external">Andrew Patton&#x2019;s MATLAB code page</a><ul>
<li>This page contains some of the MATLAB code I&#x2019;ve written during the course of my research. If you find any mistakes or bugs in the code please let me know.<br>This code is being released under a BSD license, which means that you can do pretty much what ever you want with it, including make money by selling it.</li>
</ul>
</li>
<li><a href="http://www.datatau.com/" target="_blank" rel="external">Datatau</a><ul>
<li>Hacker News for Data</li>
</ul>
</li>
<li><a href="http://sebastianruder.com/ - open" target="_blank" rel="external">Blog of Sebastian Ruder</a><ul>
<li>I&#x2019;m a PhD student in Natural Language Processing and a research scientist at AYLIEN. I blog about Machine Learning, Deep Learning, NLP, and startups.</li>
</ul>
</li>
<li><a href="http://videolectures.net/site/search/?q=MLSS" target="_blank" rel="external">Videolecture net: Machine learning</a><ul>
<li><a href="https://en.wikipedia.org/wiki/VideoLectures.net" target="_blank" rel="external">VideoLectures.net<i class="fa fa-wikipedia" aria-hidden="true"></i></a> is the world&#x2019;s biggest academic online video repository with 14,251 video lectures delivered by 10,763 presenters since 2006. It is hosted at Jozef Stefan Institute in Slovenia, Europe. All content is released under the Creative Commons Attribution-Noncommercial-No Derivative Works 3.0</li>
</ul>
</li>
<li><a href="http://www.dataschool.io/" target="_blank" rel="external">Dataschool</a><ul>
<li>You&#x2019;re trying to launch your career in data science, and I want to help you reach that goal! My name is Kevin Markham. I&#x2019;m a data scientist and a teacher.</li>
</ul>
</li>
<li>A Machine Learning-Based Trading Strategy Using Sentiment Analysis Data by Tucker Balch (Lucena Research): <a href="https://www.youtube.com/watch?v=kxwHv-12E7A" target="_blank" rel="external">Videos</a></li>
<li><del><a href="https://www.youtube.com/playlist?list=PLBv09BD7ez_7beI0_fuE96lSbsr_8K8YD" target="_blank" rel="external">Expectation Maximization Algorithm by Victor Lavrenko</a></del> (20171017)</li>
<li><a href="https://machinelearningmastery.com/start-here/" target="_blank" rel="external">Machine Learning Mastery-Start Here!</a><ul>
<li>Do You Need Help Getting Started with Applied Machine Learning? This is The Step-by-Step Guide that You&#x2019;ve Been Looking For!</li>
</ul>
</li>
<li><a href="https://www.dataquest.io/" target="_blank" rel="external">DATAQUEST:Learn real-world data science skills</a><ul>
<li>Our practical approach teaches you Data Science using interactive coding challenges.</li>
<li>You&#x2019;ll learn how to think like a data scientist. You&#x2019;ll learn how to solve problems like a data scientist. You&#x2019;ll work on the same projects a data scientist works on. And eventually, you&#x2019;ll become a data scientist.</li>
</ul>
</li>
<li><a href="https://homes.cs.washington.edu/~tqchen/projects.html" target="_blank" rel="external">Tianqi Chen&#x2019;s Project</a><ul>
<li>As a large-scale machine learning researcher, I like to build real things that can be used in production. I build some very widely used machine learning and systems packages. I am initiator DMLC group, to make large-scale machine learning widely available to the community. Here I list projects that I created and heavily involved in. Some of these projects are also great example of large-scale machine learning research. Most of these projects are actively developed and maintained as open source package. I am honored to work with many outstanding collaborators on these projects.</li>
</ul>
</li>
<li><a href="https://www.dataquest.io/blog/kaggle-getting-started/" target="_blank" rel="external">Getting Started with Kaggle: House Prices Competition</a><ul>
<li>Founded in 2010, Kaggle is a Data Science platform where users can share, collaborate, and compete. One key feature of Kaggle is &#x201C;Competitions&#x201D;, which offers users the ability to practice on real world data and to test their skills with, and against, an international community.</li>
</ul>
</li>
<li><a href="https://honglangwang.wordpress.com/2014/12/30/machine-learning-books-suggested-by-michael-i-jordan-from-berkeley/" target="_blank" rel="external">Honglang Wang&#x2019;s Blog|Machine Learning Books Suggested by <strong>Michael I. Jordan</strong> from Berkeley</a><ul>
<li>There has been a Machine Learning (ML) reading list of books in hacker news for a while, where Professor Michael I. Jordan recommend some books to start on ML for people who are going to devote many decades of their lives to the field, and who want to get to the research frontier fairly quickly. Recently he articulated the relationship between CS and Stats amazingly well in his recent reddit AMA, in which he also added some books that dig still further into foundational topics. I just list them here for people&#x2019;s convenience and my own reference.</li>
</ul>
</li>
</ul>
<ul>
<li><a href="http://www.liaoxuefeng.com/" target="_blank" rel="external">&#x5ED6;&#x96EA;&#x5CF0;&#x7684;&#x5B98;&#x65B9;&#x7F51;&#x7AD9;</a>&#xFF1A;<a href="https://www.linkedin.com/in/liaoxuefeng/?locale=zh_CN" target="_blank" rel="external">Michael Liao</a></li>
<li><a href="http://xccds1977.blogspot.hk/" target="_blank" rel="external">&#x6570;&#x636E;&#x79D1;&#x5B66;&#x4E2D;&#x7684;R&#x548C;Python</a><ul>
<li><a href="http://xccds1977.blogspot.hk/2013/01/blog-post.html" target="_blank" rel="external">&#x5982;&#x4F55;&#x5B66;&#x4E60;&#x6570;&#x636E;&#x79D1;&#x5B66;</a></li>
<li><a href="http://xccds1977.blogspot.hk/2011/11/data-science.html" target="_blank" rel="external">&#x4EC0;&#x4E48;&#x662F;&#x6570;&#x636E;&#x79D1;&#x5B66;(Data Science)</a></li>
<li><a href="http://xccds1977.blogspot.hk/2014/10/python.html" target="_blank" rel="external">python&#x7684;&#x6570;&#x636E;&#x79D1;&#x5B66;&#x8D44;&#x6E90;</a></li>
<li><a href="http://xccds1977.blogspot.hk/2015/11/blog-post.html" target="_blank" rel="external">&#x6DF1;&#x5EA6;&#x5B66;&#x4E60;&#x5165;&#x95E8;&#x8D44;&#x6E90;&#x7D22;&#x5F15;</a></li>
<li><a href="http://xccds1977.blogspot.hk/2013/02/r.html" target="_blank" rel="external">R&#x8BED;&#x8A00;&#x8D44;&#x6E90;</a></li>
</ul>
</li>
<li><a href="http://www.cnblogs.com/siegfang/" target="_blank" rel="external">&#x535A;&#x5BA2;&#x56ED;|&#x5B66;&#x98DE; &#x4ECE;&#x5760;&#x843D;&#x5F00;&#x59CB;</a></li>
<li><a href="https://cos.name/" target="_blank" rel="external">&#x7EDF;&#x8BA1;&#x4E4B;&#x90FD;</a><ul>
<li>&#x7EDF;&#x8BA1;&#x4E4B;&#x90FD;&#xFF08;Capital of Statistics&#xFF0C;&#x7B80;&#x79F0; COS&#xFF09;&#x6210;&#x7ACB;&#x4E8E; 2006 &#x5E74; 5 &#x6708; 19 &#x65E5;&#xFF0C;&#x662F;&#x4E00;&#x4E2A;&#x65E8;&#x5728;&#x63A8;&#x5E7F;&#x4E0E;&#x5E94;&#x7528;&#x7EDF;&#x8BA1;&#x5B66;&#x77E5;&#x8BC6;&#x7684;&#x7F51;&#x7AD9;&#x548C;&#x793E;&#x533A;&#x3002;&#x7EDF;&#x8BA1;&#x4E4B;&#x90FD;&#x7F51;&#x7AD9;&#x6700;&#x521D;&#x7531;&#x8C22;&#x76CA;&#x8F89;&#x521B;&#x529E;&#xFF0C;&#x73B0;&#x4EFB;&#x7406;&#x4E8B;&#x4F1A;&#x4E3B;&#x5E2D;&#x4E3A;&#x51AF;&#x51CC;&#x79C9;&#xFF0C;&#x73B0;&#x7531;&#x4E16;&#x754C;&#x5404;&#x5730;&#x7684;&#x4F17;&#x591A;&#x5FD7;&#x613F;&#x8005;&#x5171;&#x540C;&#x7BA1;&#x7406;&#x7EF4;&#x62A4;&#x3002;</li>
</ul>
</li>
<li><a href="http://rec-sys.net/forum.php" target="_blank" rel="external">&#x63A8;&#x8350;&#x7CFB;&#x7EDF;&#x8BBA;&#x575B;</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/23036112" target="_blank" rel="external">&#x77E5;&#x4E4E;|&#x63A8;&#x8350;&#x7CFB;&#x7EDF;&#x5E38;&#x7528;&#x7684;&#x63A8;&#x8350;&#x7B97;&#x6CD5;</a></li>
<li>&#x793E;&#x4F1A;&#x8BA1;&#x7B97;:<a href="http://www.sohu.com/a/118411822_473283" target="_blank" rel="external">&#x6DF1;&#x5EA6;&#x5B66;&#x4E60;&#x4E0E;&#x793E;&#x4F1A;&#x8BA1;&#x7B97;&#xFF08;PPT)&#xFF0C;&#x6E05;&#x534E;NLP&#x5B9E;&#x9A8C;&#x5BA4;&#x5218;&#x77E5;&#x8FDC;</a></li>
</ul>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/23561159" target="_blank" rel="external">Python&#x5B66;&#x4E60;&#x8DEF;&#x5F84;&#x53CA;&#x7EC3;&#x624B;&#x9879;&#x76EE;&#x5408;&#x96C6;</a><ul>
<li>&#x5B9E;&#x9A8C;&#x697C;|Python&#x57FA;&#x4E8E;&#x5171;&#x73B0;&#x63D0;&#x53D6;&#x300A;&#x91DC;&#x5C71;&#x884C;&#x300B;&#x4EBA;&#x7269;&#x5173;&#x7CFB;<ul>
<li><a href="https://www.shiyanlou.com/courses/677/labs/2202/document" target="_blank" rel="external">Document</a></li>
<li><a href="http://blog.forec.cn/2016/10/03/co-occurrence-structure-capture/" target="_blank" rel="external">Network Mining Based On Co-occurrence</a></li>
</ul>
</li>
<li><a href="https://www.shiyanlou.com/courses/633" target="_blank" rel="external">&#x5B9E;&#x9A8C;&#x697C;|&#x57FA;&#x4E8E; Flask &#x4E0E; MySQL &#x5B9E;&#x73B0;&#x756A;&#x5267;&#x63A8;&#x8350;&#x7CFB;&#x7EDF;</a><ul>
<li><a href="https://www.shiyanlou.com/courses/633/labs/2090/document" target="_blank" rel="external">Document</a></li>
</ul>
</li>
<li>Kaggle&#x5165;&#x95E8;&#xFF1A;&#x6CF0;&#x5766;&#x5C3C;&#x514B;&#x53F7;&#x5E78;&#x5B58;&#x8005;&#x9879;&#x76EE;<ul>
<li><a href="https://github.com/agconti/kaggle-titanic" target="_blank" rel="external">agconti/kaggle-titanic<i class="fa fa-github" aria-hidden="true"></i></a></li>
<li><a href="http://oldblog.fuyangzhen.com/bootstrap/blog/00147109529011688ef607e9c6b4aa291510e3eaaa9b595000" target="_blank" rel="external">&#x6570;&#x636E;&#x5206;&#x6790;&#xFF5C;Kaggle&#xFF5C;&#x6CF0;&#x5766;&#x5C3C;&#x514B;&#x53F7;&#x5E78;&#x5B58;&#x8005;&#x9879;&#x76EE;</a></li>
</ul>
</li>
<li><a href="https://www.shiyanlou.com/courses/593" target="_blank" rel="external">&#x5B9E;&#x9A8C;&#x697C;|&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x5B9E;&#x73B0;&#x624B;&#x5199;&#x5B57;&#x7B26;&#x8BC6;&#x522B;&#x7CFB;&#x7EDF;</a></li>
</ul>
</li>
<li>Python Scrapy: <a href="https://scrapy.org" target="_blank" rel="external">&#x4E0B;&#x8F7D;&#x5730;&#x5740;</a>, <a href="http://www.jb51.net/article/48607.htm" target="_blank" rel="external">&#x5B89;&#x88C5;&#x53CA;&#x4F7F;&#x7528;&#x6B65;&#x9AA4;</a>, <a href="http://www.jb51.net/article/57183.htm" target="_blank" rel="external">&#x64CD;&#x7EB5;&#x6307;&#x5357;</a></li>
<li>NITK: &#x81EA;&#x7136;&#x8BED;&#x8A00;&#x5904;&#x7406;&#x7684;Python&#x5F00;&#x6E90;&#x5DE5;&#x5177;: <a href="http://www.nltk.org" target="_blank" rel="external">&#x76F8;&#x5173;&#x5DE5;&#x5177;&#x5305;&#x548C;&#x6559;&#x7A0B;&#x53EF;&#x4E0B;&#x8F7D;</a><a href="http://blog.csdn.net/huyoo/article/details/12188573" target="_blank" rel="external">&#x4E2D;&#x6587;&#x4F7F;&#x7528;&#x4ECB;&#x7ECD;</a>, <a href="http://blog.csdn.net/huang2009303513/article/details/14498473" target="_blank" rel="external">&#x6709;&#x52A9;&#x4E8E;&#x4E86;&#x89E3;&#x5404;&#x4E2A;&#x5305;&#x7684;&#x610F;&#x601D;&#x548C;&#x4F5C;&#x7528;&#x7684;&#x94FE;&#x63A5;</a></li>
<li><a href="https://github.com/egrcc/zhihu-python - zhihu-python&#x83B7;&#x53D6;&#x77E5;&#x4E4E;&#x4FE1;&#x606F;" target="_blank" rel="external">zhihu-python&#xFF1A;&#x83B7;&#x53D6;&#x77E5;&#x4E4E;&#x4FE1;&#x606F;<i class="fa fa-github" aria-hidden="true"></i></a><ul>
<li>&#x4ECB;&#x7ECD;&#xFF1A;zhihu-python &#x91C7;&#x7528; Python2.7 &#x7F16;&#x5199;&#xFF0C;&#x7528;&#x6765;&#x65B9;&#x4FBF;&#x5730;&#x83B7;&#x53D6;&#x77E5;&#x4E4E;&#x4E0A;&#x5404;&#x79CD;&#x5185;&#x5BB9;&#x7684;&#x4FE1;&#x606F;&#xFF0C;&#x5E76;&#x4E14;&#x53EF;&#x4EE5;&#x65B9;&#x4FBF;&#x5730;&#x5C06;&#x7B54;&#x6848;&#x5907;&#x4EFD;&#x5BFC;&#x51FA;&#x4E3A; txt &#x6216; markdown &#x6587;&#x4EF6;&#x3002;&#x7531;&#x4E8E;&#x77E5;&#x4E4E;&#x5B98;&#x65B9;&#x76EE;&#x524D;&#x6CA1;&#x6709;&#x63D0;&#x4F9B; api&#xFF0C;&#x6240;&#x4EE5;&#x6709;&#x4E86;&#x6B64;&#x9879;&#x76EE;&#x7684;&#x5B58;&#x5728;&#x3002;&#x4F7F;&#x7528; Python3 &#x7684;&#x7C7B;&#x4F3C;&#x9879;&#x76EE;&#x53EF;&#x4EE5;&#x53C2;&#x89C1;&#xFF1A;zhihu-py3 &#x3002;&#x4F7F;&#x7528; PHP &#x7684;&#x7C7B;&#x4F3C;&#x9879;&#x76EE;&#x53EF;&#x4EE5;&#x53C2;&#x89C1;&#xFF1A;zhihu-php &#x3002;&#x4F7F;&#x7528; Go &#x7684;&#x7C7B;&#x4F3C;&#x9879;&#x76EE;&#x53EF;&#x4EE5;&#x53C2;&#x89C1;&#xFF1A;zhihu-go &#x3002;</li>
</ul>
</li>
<li><a href="http://www.open-open.com/lib/view/open1428112201271.html" target="_blank" rel="external">&#x6DF1;&#x5EA6;&#x5F00;&#x6E90;OPEN&#x7ECF;&#x9A8C;&#xFF1A;&#x673A;&#x5668;&#x5B66;&#x4E60;(Machine Learning)&amp;&#x6DF1;&#x5EA6;&#x5B66;&#x4E60;(Deep Learning)&#x8D44;&#x6599;</a></li>
<li><a href="http://www.cnblogs.com/mldllearningforbai/p/5316728.html" target="_blank" rel="external">[&#x8F6C;]&#x673A;&#x5668;&#x5B66;&#x4E60;&#x5165;&#x95E8;</a></li>
<li><a href="http://www.cnblogs.com/mldllearningforbai/p/5351584.html" target="_blank" rel="external">[&#x8F6C;]&#x673A;&#x5668;&#x5B66;&#x4E60;&#x7ECF;&#x5178;&#x4E66;&#x7C4D;</a></li>
</ul>
<hr>
<h1 id="Finance-amp-Economics"><a href="#Finance-amp-Economics" class="headerlink" title="Finance &amp; Economics"></a>Finance &amp; Economics</h1><p><img src="/images/Financial_Theories.jpg" alt="Financial Theories"></p>
<p><img src="/images/Investment_Desicion_Making.jpg" alt="Investment Desicion Making Problem"></p>
<h2 id="Courses-4"><a href="#Courses-4" class="headerlink" title="Courses"></a>Courses</h2><ul>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{G}$</strong>MIT 15.401 - Finance Theory I by <strong>Andrew Lo</strong>, Fall 2008</font>: 1h10min/Lec, 20 Lectures, <a href="https://ocw.mit.edu/courses/sloan-school-of-management/15-401-finance-theory-i-fall-2008/" target="_blank" rel="external">Course Page</a>, <a href="https://www.youtube.com/playlist?list=PLUl4u3cNGP63B2lDhyKOsImI7FjCf6eDW" target="_blank" rel="external">Videos<i class="fa fa-youtube" aria-hidden="true"></i></a><ul>
<li>Course Description: This course introduces the core theory of modern financial economics and financial management, with a focus on capital markets and investments. Topics include functions of capital markets and financial intermediaries, asset valuation, fixed-income securities, common stocks, capital budgeting, diversification and portfolio selection, equilibrium pricing of risky assets, the theory of efficient markets, and an introduction to derivatives and options.</li>
</ul>
</li>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{G}$</strong>MIT 15.402 - Finance Theory II by <strong>Dirk Jenter &amp; Katharina Lewellen</strong>, Spring 2003</font>: <a href="https://ocw.mit.edu/courses/sloan-school-of-management/15-402-finance-theory-ii-spring-2003/index.htm" target="_blank" rel="external">Course Page</a><ul>
<li>Prerequisites: Finance Theory I (15.401). It will also help if you know some basic economics and accounting.</li>
<li>Course Description: The objective of this course is to learn the financial tools needed to make good business decisions. The course presents the basic insights of corporate finance theory, but emphasizes the application of theory to real business decisions. Each session involves class discussion, some centered on lectures and others around business cases.</li>
</ul>
</li>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{UG}$</strong>Economic 421 - Econometrics by <strong>Mark Thoma</strong></font>: 1h20min/Lec, 19 Lectures, <a href="http://economistsview.typepad.com/economics421/winter-2010/" target="_blank" rel="external">Course Page</a>, <a href="https://www.youtube.com/playlist?list=PLD15D38DC7AA3B737" target="_blank" rel="external">Videos<i class="fa fa-youtube" aria-hidden="true"></i></a><ul>
<li>Course Description: We covered the following topics in the course: Assumption required for estimates to be BLUE, Hypothesis testing, Heteroskedasticity, Autocorrelation, Testing for ARCH errors, Stochastic Regressors and Measurement Errors, Simultaneous equation models, Multicollinearity, Specification tests, Qualitative and limited dependent variables, Maximum likelihood etc.</li>
</ul>
</li>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{UG}$</strong>Yale Econ 159 - Game Theory by <strong>Ben Polak</strong></font>: <a href="https://oyc.yale.edu/economics/econ-159" target="_blank" rel="external">Course Page</a>, <a href="https://www.youtube.com/playlist?list=PLDv_VQy7kufe5YMXjf6LkacYBrSjlXQB8" target="_blank" rel="external">Videos<i class="fa fa-youtube" aria-hidden="true"></i></a><ul>
<li>Prerequisites: Introductory microeconomics (115 or equivalent) is required. Intermediate micro (150/2) is not required, but it is recommended. We will use calculus (mostly one variable) in this course. We will also refer to ideas like probability and expectation. Some may prefer to take the course next academic year once they have more background. Students who have already taken Econ 156b should not enroll in this class.</li>
<li>Course Description: This course is an introduction to game theory and strategic thinking. Ideas such as dominance, backward induction, Nash equilibrium, evolutionary stability, commitment, credibility, asymmetric information, adverse selection, and signaling are discussed and applied to games played in class and to examples drawn from economics, politics, the movies, and elsewhere.</li>
</ul>
</li>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{UG}$</strong>NTU IM 7011 - Information Economics by Ling-Chieh Kung, Fall 2014</font>: <a href="http://www.im.ntu.edu.tw/~lckung/courses/old/IE14/" target="_blank" rel="external">Course Page</a>, <a href="https://www.youtube.com/playlist?list=PLAomEJmGfjBGoyXN_63WswMY0C-da51vo" target="_blank" rel="external">Videos<i class="fa fa-youtube" aria-hidden="true"></i></a><ul>
<li>Prerequisites: Students need to know the basic ideas of calculus, optimization, and probability. Some knowledge about game theory will be helpful.</li>
<li>Course Description: There are four modules in this course: decentralization and inefficiency, the screening theory, the signaling theory, and final project presentations. The course starts with discussions about the incentive issues in decentralized systems. We then spend most of our time studying the economics of information to understand the impacts of possessing information or being lack of information. The focus will be on adverse selection, one of the most well studied types of information asymmetry in the field of economics. We will discuss how one may screen others&#x2019; private information and signal its own private information. Finally, students&#x2019; final project presentations conclude this course.</li>
</ul>
</li>
</ul>
<h2 id="Books-2"><a href="#Books-2" class="headerlink" title="Books"></a>Books</h2><ul>
<li><a href="https://www.scu.edu/business/finance/faculty/das/" target="_blank" rel="external">Sanjiv Ranjan Das</a>, Data Science: Theories, Models, Algorithms, and Analytics: <a href="http://srdas.github.io/MLBook/" target="_blank" rel="external">GitBook<i class="fa fa-git" aria-hidden="true"></i></a></li>
</ul>
<hr>
<h1 id="Information-System"><a href="#Information-System" class="headerlink" title="Information System"></a>Information System</h1><h2 id="Courses-5"><a href="#Courses-5" class="headerlink" title="Courses"></a>Courses</h2><ul>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{G}$</strong>MIT 15.082J/6.855J/ESD.78J - Network optimization by James Orlin, Fall 2010</font>: <a href="https://ocw.mit.edu/courses/sloan-school-of-management/15-082j-network-optimization-fall-2010/index.htm" target="_blank" rel="external">Course Page</a><ul>
<li>Prerequisites: 6.251J/15.081J Introduction to Mathematical Programming or a course on data structures.</li>
<li>Course Description: 15.082J/6.855J/ESD.78J is a graduate subject in the theory and practice of network flows and its extensions. Network flow problems form a subclass of linear programming problems with applications to transportation, logistics, manufacturing, computer science, project management, and finance, as well as a number of other domains. This subject will survey some of the applications of network flows and focus on key special cases of network flow problems including the following: the shortest path problem, the maximum flow problem, the minimum cost flow problem, and the multi-commodity flow problem. We will also consider other extensions of network flow problems.</li>
</ul>
</li>
<li><font color="#1E8449" size="2.5"><strong>$\mathbb{G}$</strong>MIT 6.231 Stochastic Control Theory by Dimitri Bertsekas, Fall 2015</font>: <a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-231-dynamic-programming-and-stochastic-control-fall-2015/" target="_blank" rel="external">Course Page</a>, <a href="https://www.youtube.com/playlist?list=PLiCLbsFQNFAxOmVeqPhI5er1LGf2-L9I4" target="_blank" rel="external">Special Videos</a><ul>
<li>Prerequisites:<ul>
<li>Solid knowledge of undergraduate probability, at the level of 6.041 Probabilistic Systems Analysis and Applied Probability, especially conditional distributions and expectations, and Markov chains.</li>
<li>Mathematical maturity and the ability to write down precise and rigorous arguments are also important. A class in analysis (e.g. 18.100C Real Analysis) will be helpful, although this prerequisite will not be strictly enforced.</li>
</ul>
</li>
<li>Course Description: The course covers the basic models and solution techniques for problems of sequential decision making under uncertainty (stochastic control). We will consider optimal control of a dynamical system over both a finite and an infinite number of stages. This includes systems with finite or infinite state spaces, as well as perfectly or imperfectly observed systems. We will also discuss approximation methods for problems involving large state spaces. Applications of dynamic programming in a variety of fields will be covered in recitations.</li>
</ul>
</li>
<li>Operation Research</li>
<li>Data mining/Text mining</li>
<li>Knowledge management</li>
<li>Social media/Social computing/Social network</li>
<li>Decision Support System</li>
</ul>
<h2 id="IS-Professors-on-Data-Science"><a href="#IS-Professors-on-Data-Science" class="headerlink" title="IS Professors on Data Science"></a>IS Professors on Data Science</h2><ul>
<li><a href="http://www.u.arizona.edu/~junmingy/" target="_blank" rel="external">Junming Yin @ MIS Arizona</a></li>
<li><a href="https://mis.eller.arizona.edu/people/daniel-zeng" target="_blank" rel="external">Daniel Zeng @ MIS Arizona</a></li>
<li><a href="http://nlp.lab.arizona.edu/" target="_blank" rel="external">Gondy Leroy, Eller Fellow @ MIS Arizona</a></li>
<li><a href="https://mis.eller.arizona.edu/people/hsinchun-chen" target="_blank" rel="external">Hsinchun Chen @ MIS Arizona</a></li>
<li><a href="https://mis.eller.arizona.edu/people/sudha-ram" target="_blank" rel="external">Sudha Ram @ MIS Arizona</a></li>
<li><a href="http://jindal.utdallas.edu/faculty/vijay-mookerjee/" target="_blank" rel="external">Vijay Mookerjee @ IS UTDallas</a></li>
<li><a href="http://faculty.washington.edu/ytan/" target="_blank" rel="external">Yong TAN @ IS, Washington</a></li>
<li><a href="http://leedokyun.com/index.html" target="_blank" rel="external">Dokyun Lee @ IS&amp;Econ @ CMU</a></li>
<li><a href="http://homepage.hit.edu.cn/pages/xitongguo" target="_blank" rel="external">&#x54C8;&#x5DE5;&#x5927;&#x7535;&#x5B50;&#x5065;&#x5EB7;&#x7814;&#x7A76;&#x6240;&#xFF1A;&#x90ED;&#x7199;&#x94DC;</a></li>
<li><a href="http://sim.whu.edu.cn/sz/jsxq/5/2016-05-09/898.html" target="_blank" rel="external">&#x6B66;&#x6C49;&#x5927;&#x5B66;&#x4FE1;&#x606F;&#x7BA1;&#x7406;&#x5B66;&#x9662;&#xFF1A;&#x66FE;&#x5B50;&#x660E;</a></li>
<li><a href="http://nlp.csai.tsinghua.edu.cn/site2/index.php/zh" target="_blank" rel="external">&#x6E05;&#x534E;&#x5927;&#x5B66;&#x81EA;&#x7136;&#x8BED;&#x8A00;&#x5904;&#x7406;&#x4E0E;&#x793E;&#x4F1A;&#x4EBA;&#x6587;&#x8BA1;&#x7B97;&#x5B9E;&#x9A8C;&#x5BA4;</a></li>
<li><a href="http://idke.ruc.edu.cn/index_cn.htm" target="_blank" rel="external">&#x4E2D;&#x56FD;&#x4EBA;&#x6C11;&#x5927;&#x5B66;&#xFF1A;&#x7F51;&#x7EDC;&#x4E0E;&#x79FB;&#x52A8;&#x6570;&#x636E;&#x7BA1;&#x7406;&#x5B9E;&#x9A8C;&#x5BA4;</a></li>
</ul>
<h2 id="Resources"><a href="#Resources" class="headerlink" title="Resources"></a>Resources</h2><ul>
<li>The Structural Modeling and Machine Learning Applications for Research on Technology Workshop: <a href="http://faculty.washington.edu/ytan/SMART2017/SMART2017.html" target="_blank" rel="external">2017 Home Page<i class="fa fa-home" aria-hidden="true"></i></a></li>
<li><a href="http://leedokyun.com/deep-learning-reading-list.html" target="_blank" rel="external">Dokyun Lee&#x2019;S Deep Learning Resources</a></li>
<li><a href="http://bt.tepper.cmu.edu/links/" target="_blank" rel="external">CMU Business Technology Group-Links</a></li>
</ul>
<hr>
<h1 id="Others-2"><a href="#Others-2" class="headerlink" title="Others"></a>Others</h1><ul>
<li><del>Github</del></li>
<li><del>Hexo</del></li>
<li><del>Markdown</del></li>
<li><del>LatTex</del></li>
</ul>
<hr>
<h1 id="Tools"><a href="#Tools" class="headerlink" title="Tools"></a>Tools</h1><p><strong>Learn Anything</strong> is an Open Source Website built by community to Learn Anything with Interactive Maps: <a href="https://github.com/learn-anything/learn-anything#learn-anything---" target="_blank" rel="external">GitHub<i class="fa fa-github" aria-hidden="true"></i></a>, <a href="https://learn-anything.xyz/" target="_blank" rel="external">Home Page<i class="fa fa-external-link" aria-hidden="true"></i></a></p>

      
    </div>
    
    
    

    <div>
    
      <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------End of post<i class="fa fa-flag"></i>Thanks for your time-------------</div>
    
</div>

    
    </div>

    
      <div>
        <div id="wechat_subscriber" style="display: block; padding: 10px 0; margin: 20px auto; width: 100%; text-align: center">
    <img id="wechat_subscriber_qcode" src="/qrcode_for_wechat_bdg.jpg" alt="BDG飽蠹閣 wechat" style="width: 200px; max-width: 100%;"/>
    <div>Enjoy it? Subscribe to my blog by scanning my public wechat account</div>
</div>

      </div>
    

    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Post author:</strong>
    BDG飽蠹閣
  </li>
  <li class="post-copyright-link">
    <strong>Post link:</strong>
    <a href="http://blog.baoduge.com/2017_Study_Plan/" title="2017-18 Open-Course Challenges: Data Science on IS">http://blog.baoduge.com/2017_Study_Plan/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice: </strong>
    All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> unless stating additionally.
  </li>
</ul>

      </div>
    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/machine-learning/" rel="tag"># machine_learning</a>
          
            <a href="/tags/natural-language-processing/" rel="tag"># natural_language_processing</a>
          
            <a href="/tags/deep-learning/" rel="tag"># deep_learning</a>
          
            <a href="/tags/information-system/" rel="tag"># information_system</a>
          
            <a href="/tags/markdown-table/" rel="tag"># markdown_table</a>
          
            <a href="/tags/image/" rel="tag"># image</a>
          
            <a href="/tags/github/" rel="tag"># github</a>
          
            <a href="/tags/hexo-pdf/" rel="tag"># hexo_pdf</a>
          
            <a href="/tags/finance/" rel="tag"># finance</a>
          
            <a href="/tags/economics/" rel="tag"># economics</a>
          
            <a href="/tags/mathematics/" rel="tag"># mathematics</a>
          
            <a href="/tags/statistics/" rel="tag"># statistics</a>
          
            <a href="/tags/artificial-intelligence/" rel="tag"># artificial_intelligence</a>
          
            <a href="/tags/open-course/" rel="tag"># open_course</a>
          
            <a href="/tags/span-id-anchor/" rel="tag"># span_id_anchor</a>
          
        </div>
      

      
      
        <div class="post-widgets">
        
          <div class="wp_rating">
            <div style="color:rgb(0, 0, 0); font-size:13px; letter-spacing:3px;">Rate this post</div>
            <div id="wpac-rating"></div>
          </div>
        

        
        </div>
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/CUHK_DSME6622_Lec2/" rel="next" title="Experiments in IS research">
                <i class="fa fa-chevron-left"></i> Experiments in IS research
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017_Logs_Questions/" rel="prev" title="NEVERLAND - Blog Logs">
                NEVERLAND - Blog Logs <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
      <div id="sidebar-dimmer"></div>
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          
            <img class="site-author-image" itemprop="image"
              src="http://baoduge.com/My/avatar.gif"
              alt="BDG飽蠹閣" />
          
            <p class="site-author-name" itemprop="name">BDG飽蠹閣</p>
            <p class="site-description motion-element" itemprop="description"></p>
        </div>

        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
            
              <a href="/archives">
            
                <span class="site-state-item-count">31</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">2</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">85</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/vincent27hugh" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>GitHub</a>
              </span>
            
              <span class="links-of-author-item">
                <a href="mailto:huweihugh@gmail.com" target="_blank" title="E-Mail">
                  
                    <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              Links
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://baoduge.com/" title="飽蠹閣BAODUGE" target="_blank">飽蠹閣BAODUGE</a>
                </li>
              
            </ul>
            
            <div id="days"></div>
</script>
<script language="javascript">
function show_date_time(){
window.setTimeout("show_date_time()", 1000);
BirthDay=new Date("09/08/2017 00:00:00");
today=new Date();
timeold=(today.getTime()-BirthDay.getTime());
sectimeold=timeold/1000
secondsold=Math.floor(sectimeold);
msPerDay=24*60*60*1000
e_daysold=timeold/msPerDay
daysold=Math.floor(e_daysold);
e_hrsold=(e_daysold-daysold)*24;
hrsold=setzero(Math.floor(e_hrsold));
e_minsold=(e_hrsold-hrsold)*60;
minsold=setzero(Math.floor((e_hrsold-hrsold)*60));
seconds=setzero(Math.floor((e_minsold-minsold)*60));
document.getElementById('days').innerHTML="Has been runing "+daysold+" Days "+hrsold+" Hours "+minsold+" Min "+seconds+" Sec ";
}
function setzero(i){
if (i<10)
{i="0" + i};
return i;
}
show_date_time();
</script>

          </div>
        

        <div id="days"></div>
</script>
<script language="javascript">
function show_date_time(){
window.setTimeout("show_date_time()", 1000);
BirthDay=new Date("09/08/2017 00:00:00");
today=new Date();
timeold=(today.getTime()-BirthDay.getTime());
sectimeold=timeold/1000
secondsold=Math.floor(sectimeold);
msPerDay=24*60*60*1000
e_daysold=timeold/msPerDay
daysold=Math.floor(e_daysold);
e_hrsold=(e_daysold-daysold)*24;
hrsold=setzero(Math.floor(e_hrsold));
e_minsold=(e_hrsold-hrsold)*60;
minsold=setzero(Math.floor((e_hrsold-hrsold)*60));
seconds=setzero(Math.floor((e_minsold-minsold)*60));
document.getElementById('days').innerHTML="Has been runing "+daysold+" Days "+hrsold+" Hours "+minsold+" Min "+seconds+" Sec ";
}
function setzero(i){
if (i<10)
{i="0" + i};
return i;
}
show_date_time();
</script>


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Statement"><span class="nav-number">1.</span> <span class="nav-text">Statement</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#General-Information"><span class="nav-number">2.</span> <span class="nav-text">General Information</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Mathematics"><span class="nav-number">3.</span> <span class="nav-text">Mathematics</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Courses"><span class="nav-number">3.1.</span> <span class="nav-text">Courses</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Linear-Algebra-amp-Vector-Calculus"><span class="nav-number">3.1.1.</span> <span class="nav-text">Linear Algebra & Vector Calculus</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Functional-Analysis"><span class="nav-number">3.1.2.</span> <span class="nav-text">Functional Analysis</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Numerical-Analysis"><span class="nav-number">3.1.3.</span> <span class="nav-text">Numerical Analysis</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Stochastic-Analysis"><span class="nav-number">3.1.4.</span> <span class="nav-text">Stochastic Analysis</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Optimization"><span class="nav-number">3.1.5.</span> <span class="nav-text">Optimization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Financial-Mathematics"><span class="nav-number">3.1.6.</span> <span class="nav-text">Financial Mathematics</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Computer-Science"><span class="nav-number">4.</span> <span class="nav-text">Computer Science</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Courses-1"><span class="nav-number">4.1.</span> <span class="nav-text">Courses</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Data-structure-and-algorithms"><span class="nav-number">4.1.1.</span> <span class="nav-text">Data structure and algorithms</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Statistics-amp-Probability"><span class="nav-number">5.</span> <span class="nav-text">Statistics & Probability</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Courses-2"><span class="nav-number">5.1.</span> <span class="nav-text">Courses</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Basics"><span class="nav-number">5.1.1.</span> <span class="nav-text">Basics</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Statistical-Learning"><span class="nav-number">5.1.2.</span> <span class="nav-text">Statistical Learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Others"><span class="nav-number">5.1.3.</span> <span class="nav-text">Others</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Books"><span class="nav-number">5.2.</span> <span class="nav-text">Books</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Machine-Learning"><span class="nav-number">6.</span> <span class="nav-text">Machine Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Courses-3"><span class="nav-number">6.1.</span> <span class="nav-text">Courses</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Artificial-Intelligence"><span class="nav-number">6.1.1.</span> <span class="nav-text">Artificial Intelligence</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Machine-Learning-1"><span class="nav-number">6.1.2.</span> <span class="nav-text">Machine Learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Deep-Learning"><span class="nav-number">6.1.3.</span> <span class="nav-text">Deep Learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Natural-Language-Processing"><span class="nav-number">6.1.4.</span> <span class="nav-text">Natural Language Processing</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Probabilistic-Graphical-Models"><span class="nav-number">6.1.5.</span> <span class="nav-text">Probabilistic Graphical Models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Applications"><span class="nav-number">6.1.6.</span> <span class="nav-text">Applications</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Other-courses"><span class="nav-number">6.1.7.</span> <span class="nav-text">Other courses</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Books-1"><span class="nav-number">6.2.</span> <span class="nav-text">Books</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Introduction-to-Artificial-Intelligence"><span class="nav-number">6.2.1.</span> <span class="nav-text">Introduction to Artificial Intelligence</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Machine-Learning-2"><span class="nav-number">6.2.2.</span> <span class="nav-text">Machine Learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Deep-Learning-1"><span class="nav-number">6.2.3.</span> <span class="nav-text">Deep Learning:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Natural-Language-Processing-1"><span class="nav-number">6.2.4.</span> <span class="nav-text">Natural Language Processing</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Information-Retrieval"><span class="nav-number">6.2.5.</span> <span class="nav-text">Information Retrieval</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Recommender-System"><span class="nav-number">6.2.6.</span> <span class="nav-text">Recommender System</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Others-1"><span class="nav-number">6.2.7.</span> <span class="nav-text">Others</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#List-of-list"><span class="nav-number">6.3.</span> <span class="nav-text">List of list</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Notes-Blog-Talks-and-so-on"><span class="nav-number">6.4.</span> <span class="nav-text">Notes, Blog, Talks and so on</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Finance-amp-Economics"><span class="nav-number">7.</span> <span class="nav-text">Finance & Economics</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Courses-4"><span class="nav-number">7.1.</span> <span class="nav-text">Courses</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Books-2"><span class="nav-number">7.2.</span> <span class="nav-text">Books</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Information-System"><span class="nav-number">8.</span> <span class="nav-text">Information System</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Courses-5"><span class="nav-number">8.1.</span> <span class="nav-text">Courses</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#IS-Professors-on-Data-Science"><span class="nav-number">8.2.</span> <span class="nav-text">IS Professors on Data Science</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Resources"><span class="nav-number">8.3.</span> <span class="nav-text">Resources</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Others-2"><span class="nav-number">9.</span> <span class="nav-text">Others</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Tools"><span class="nav-number">10.</span> <span class="nav-text">Tools</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2017 &mdash; 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">BDG飽蠹閣</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div>

  <span class="post-meta-divider">|</span>

  <div class="theme-info">Theme &mdash; <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.2</div>

  <div>
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  Total Visits: <span id="busuanzi_value_site_pv"></span> &nbsp&nbsp&nbsp
  Total Visitors: <span id="busuanzi_value_site_uv"></span>
</div>




        







        
      </div>
    </footer>

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>

  
  <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  

  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>


  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  

    
      <script id="dsq-count-scr" src="https://baoduge.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://blog.baoduge.com/2017_Study_Plan/';
          this.page.identifier = '2017_Study_Plan/';
          this.page.title = '2017-18 Open-Course Challenges: Data Science on IS';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://baoduge.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  













  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('5');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("EkPIkYolbkzKanbzn4mNKpf7-gzGzoHsz", "lP1WbRwpyyu9r1NAqva2xWmS");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  
  <script type="text/javascript">
  wpac_init = window.wpac_init || [];
  wpac_init.push({widget: 'Rating', id: 7746,
    el: 'wpac-rating',
    color: 'fc6423'
  });
  (function() {
    if ('WIDGETPACK_LOADED' in window) return;
    WIDGETPACK_LOADED = true;
    var mc = document.createElement('script');
    mc.type = 'text/javascript';
    mc.async = true;
    mc.src = '//embed.widgetpack.com/widget.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(mc, s.nextSibling);
  })();
  </script>


  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
